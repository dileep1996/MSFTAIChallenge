{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYpR5CXhVfD1"
   },
   "source": [
    "# Loading datasets and creating folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "LPcElNkpTp1E",
    "outputId": "42d28b13-1715-4ee5-81e6-c79405a58bb7"
   },
   "outputs": [],
   "source": [
    "!pip install cntk\n",
    "!pip install -U scikit-learn\n",
    "!pip install zipfile36\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uF5axD1FeFOJ"
   },
   "outputs": [],
   "source": [
    "!mkdir msaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PUyrlbH2Ndkr",
    "outputId": "0d224f5f-d340-4ae7-c94a-e15a6e4c97f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/ontology/Muralidhar/HyperQA/msaic\n",
      "biLSTM_Attention    glove.6B.300d.txt\t       hyperQA_glove_500\r\n",
      "data.zip\t    glove.840B.300d.txt        hyperQA_glove_elmo_100\r\n",
      "elmo2\t\t    hyperQA_elmo_100dummy.txt  hyperQA_glove_elmo_20\r\n",
      "glove.42B.300d.txt  hyperQA_elmo_20dummy.txt\r\n"
     ]
    }
   ],
   "source": [
    "%cd msaic\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "6BEV8YFdYDOP",
    "outputId": "378c31ec-6cd3-4299-cef3-f86e11a8280b"
   },
   "outputs": [],
   "source": [
    "!wget https://competitions.codalab.org/my/datasets/download/2c6a99a5-b071-4f1d-a3b1-d49a923e0c68 \n",
    "!mv 2c6a99a5-b071-4f1d-a3b1-d49a923e0c68 data.zip\n",
    "#!mv data.zip msaic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "oIc5WKkpaosL",
    "outputId": "8a769466-7133-4658-fe40-bc75e46447ca"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!mv glove.6B.zip msaic/\n",
    "glove_zip = r'glove.6B.zip';\n",
    "\n",
    "#!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "#!mv glove.840B.300d.zip msaic/\n",
    "#glove_zip = r'glove.840B.300d.zip';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rfcJb2VemQD"
   },
   "outputs": [],
   "source": [
    "#!wget https://competitions.codalab.org/my/datasets/download/a6b5bd16-db6f-4cbe-9670-04ecb7504a7a\n",
    "#!mv a6b5bd16-db6f-4cbe-9670-04ecb7504a7a Starting_Kit.zip\n",
    "#!mv Starting_Kit.zip msaic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "4NiLurmJd8Kp",
    "outputId": "a743b878-bada-45b1-8951-52e81018458a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Extracted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nzipref = zipfile.ZipFile(glove_zip, 'r')\\nzipref.extractall()\\nzipref.close()\\nprint('Glove Data Extracted')\\n\\n\\nzipref = zipfile.ZipFile('Starting_Kit.zip', 'r')\\nzipref.extractall()\\nzipref.close()\\nprint('Starting Kit unzipped')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zipref = zipfile.ZipFile('data.zip', 'r')\n",
    "zipref.extractall()\n",
    "zipref.close()\n",
    "print('Raw Data Extracted')\n",
    "\n",
    "zipref = zipfile.ZipFile(glove_zip, 'r')\n",
    "zipref.extractall()\n",
    "zipref.close()\n",
    "print('Glove Data Extracted')\n",
    "\n",
    "'''\n",
    "zipref = zipfile.ZipFile('Starting_Kit.zip', 'r')\n",
    "zipref.extractall()\n",
    "zipref.close()\n",
    "print('Starting Kit unzipped')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "SQPWL6Hutcw9",
    "outputId": "bbce9f0f-0f55-4517-8d28-f55840967ad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/ontology/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /data/ontology/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ontology/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndevice_name = tf.test.gpu_device_name()\\nif device_name != '/device:GPU:0':\\n  raise SystemError('GPU device not found')\\nprint('Found GPU at: {}'.format(device_name))\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='2';\n",
    "tf_sess_config = tf.ConfigProto()\n",
    "tf_sess_config.gpu_options.allow_growth = True\n",
    "'''\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "neuvn7Yrqo86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "print(cachedStopWords)\n",
    "\n",
    "# Utility Funcs\n",
    "def progressBar(value, endvalue, print_vars=[''], print_values=[-1], bar_length=20):\n",
    "        percent = float(value) / endvalue\n",
    "        arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "        spaces = ' ' * (bar_length - len(arrow))\n",
    "        assert(len(print_vars)==len(print_values));\n",
    "        strg = '';\n",
    "        for var, val in zip(print_vars,print_values):\n",
    "          strg+=' {}: '.format(var.upper());\n",
    "          strg+='{:5f} |'.format(val);\n",
    "        sys.stdout.write(\"\\rPercent: [{0}][{1}/{2}] {3}% || {4} \".format(arrow + spaces, int(value), int(endvalue), int(round(percent * 100)), strg))\n",
    "        sys.stdout.flush()\n",
    "        return\n",
    "#progressBar(5,25,['loss','acc'],[5,6])\n",
    "\n",
    "def cleanText_None(str):\n",
    "  new_str = [x for x in str if x]\n",
    "  return new_str, len(new_str)\n",
    "\n",
    "def cleanText_Simple(str):\n",
    "  new_str = [x.lower() for x in re.split('\\W+', str) if x]\n",
    "  return new_str, len(new_str)\n",
    "\n",
    "def cleanText_Spaces_(str_):\n",
    "  new_str = [x for x in re.split(' ', str_) if x]\n",
    "  return new_str, len(new_str)\n",
    "def cleanText_Spaces(str_):\n",
    "  new_str = [x for x in re.split(' ', str_) if x]\n",
    "  return ' '.join(new_str), len(new_str)\n",
    "\n",
    "def cleanText_StopWords_(str):\n",
    "  new_str = [x.lower() for x in re.split('\\W+', str) if (x and x not in cachedStopWords)]\n",
    "  return new_str, len(new_str)\n",
    "def cleanText_StopWords(str):\n",
    "  new_str = [x.lower() for x in re.split('\\W+', str) if (x and x not in cachedStopWords)]\n",
    "  return ' '.join(new_str), len(new_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rc_wQF_KNIBZ",
    "outputId": "8ecfefb5-2c8c-4845-b0f1-c3c3d4ebcdd1"
   },
   "outputs": [],
   "source": [
    "cleanText = cleanText_Spaces_;\n",
    "#cleanText = cleanText_Simple;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "DN8eoRjHqs5l",
    "outputId": "0298130d-52bb-46dc-9795-765dc32e1739"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5241880it [02:42, 32185.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total No.of Queries: 524193, Passages: 5241880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data and store it in a dictionary format\n",
    "data_dict = {};\n",
    "id2passage = {};\n",
    "n_queries = -1;\n",
    "n_passages = -1;\n",
    "running_query = '';\n",
    "query_len_counter = np.zeros(1000, dtype='int32');\n",
    "passage_len_counter = np.zeros(1000, dtype='int32');\n",
    "\n",
    "f = open('data.tsv','r',encoding='utf-8');\n",
    "for i, line in enumerate(tqdm(f)):\n",
    "  tokens = line.strip().split(\"\\t\")\n",
    "  query,passage,label = tokens[1],tokens[2],tokens[3];\n",
    "  _, cpassage_len = cleanText(passage);\n",
    "  passage_len_counter[np.min([999,cpassage_len])]+=1;\n",
    "  n_passages+=1;\n",
    "  id2passage[n_passages] = passage;\n",
    "  if running_query!=query:\n",
    "    running_query = query;\n",
    "    n_queries+=1;\n",
    "    data_dict[n_queries] = {};\n",
    "    _, cquery_len = cleanText(query);\n",
    "    query_len_counter[np.min([999,cquery_len])]+=1;\n",
    "    data_dict[n_queries]['query'] = query;\n",
    "    data_dict[n_queries]['query_len'] = cquery_len;\n",
    "  if int(label)==1:\n",
    "    data_dict[n_queries]['pos'] = n_passages;\n",
    "    data_dict[n_queries]['pos_len'] = cpassage_len;\n",
    "  else:\n",
    "    if 'negs' not in data_dict[n_queries]:\n",
    "      data_dict[n_queries]['negs']=[];\n",
    "      data_dict[n_queries]['negs_len']=[];\n",
    "    data_dict[n_queries]['negs'].append(n_passages);\n",
    "    data_dict[n_queries]['negs_len'].append(cpassage_len);\n",
    "print('\\nTotal No.of Queries: {}, Passages: {}'.format(n_queries+1,n_passages+1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "colab_type": "code",
    "id": "z7oRVHsjqvWW",
    "outputId": "5cf27a7a-ec30-480c-fa7e-d79d80314398"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(query_len_counter[:50]);\n",
    "plt.show()\n",
    "avg_len = np.sum([(i+1)*val for i, val in enumerate(query_len_counter)])/np.sum(query_len_counter);\n",
    "print('Avg length of queries: {}'.format(avg_len))\n",
    "plt.plot(passage_len_counter[:500]);\n",
    "plt.show()\n",
    "avg_len = np.sum([(i+1)*val for i, val in enumerate(passage_len_counter)])/np.sum(passage_len_counter);\n",
    "print('Avg length of passages: {}'.format(avg_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "yyc_6aSDqxkl",
    "outputId": "21947913-b418-461a-dbd5-63e69feb5179"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 524193/524193 [00:00<00:00, 779107.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[51949, 280200, 357523, 434822, 519532] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if any queries have no right answer!\n",
    "cnt1 = [];\n",
    "cnt2 = [];\n",
    "for id_ in tqdm(data_dict.keys()):\n",
    "  if 'pos' not in data_dict[id_].keys():\n",
    "    cnt1.append(id_)\n",
    "  if 'negs' not in data_dict[id_].keys():\n",
    "    cnt2.append(id_)\n",
    "print('');\n",
    "print(cnt1,cnt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RBnQ6mm4qz1N"
   },
   "outputs": [],
   "source": [
    "# Remove queries that have no right or at least one wrong answers\n",
    "for id_ in (cnt1+cnt2):\n",
    "  del data_dict[id_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3417
    },
    "colab_type": "code",
    "id": "lEQ1FN6ZN6wf",
    "outputId": "5b891c7f-7716-427d-92b4-b07af20877de"
   },
   "outputs": [],
   "source": [
    "# Printing some queries\n",
    "for q_id in np.random.choice(range(n_queries), size=100, replace=False):  \n",
    "    print('----------')\n",
    "    print(data_dict[q_id]['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "colab_type": "code",
    "id": "byQcSYW4q121",
    "outputId": "8810c74d-c166-44be-95ea-1aaf5842b74a"
   },
   "outputs": [],
   "source": [
    "# Printing some passages\n",
    "for i in np.random.choice(range(n_passages), size=20, replace=False):\n",
    "    print('----------')\n",
    "    print(id2passage[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7177
    },
    "colab_type": "code",
    "id": "m-UFz7hTq37p",
    "outputId": "2a5326c9-8455-4eb3-b98e-421b7ee7953a"
   },
   "outputs": [],
   "source": [
    "# Printing some queries and answers\n",
    "def printData(query_id):\n",
    "    print('**************')\n",
    "    print(data_dict[query_id]['query']);\n",
    "    print(id2passage[data_dict[query_id]['pos']]);\n",
    "    for i in data_dict[query_id]['negs']:\n",
    "        print('---------')\n",
    "        print(id2passage[i])\n",
    "        \n",
    "for q_id in np.random.choice(range(n_queries), size=20, replace=False):  \n",
    "    printData(q_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNGh0juSq6q8"
   },
   "outputs": [],
   "source": [
    "# some global vars\n",
    "TRAINING_PERCENT = 0.8\n",
    "VAL_PERCENT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ECqe-9fcq9Ju",
    "outputId": "79ee641b-d4d6-4f46-bb96-44b85bc378f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING QUERIES: 419350, VALIDATION EQURIES: 104837\n"
     ]
    }
   ],
   "source": [
    "# divide train and test\n",
    "unique_query_ids = list(data_dict.keys())\n",
    "np.random.shuffle(unique_query_ids) #in-place\n",
    "n_train = int(np.floor(len(unique_query_ids)*TRAINING_PERCENT))\n",
    "n_val = int(np.floor(len(unique_query_ids)*VAL_PERCENT))\n",
    "training_query_ids, validation_query_ids = unique_query_ids[:n_train],unique_query_ids[n_train:n_train+n_val]\n",
    "print('TRAINING QUERIES: {}, VALIDATION EQURIES: {}'.format(n_train,n_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vft5LO05q9mr"
   },
   "outputs": [],
   "source": [
    "# some global vars\n",
    "UNKNOWN_GLOVE = '_unk_';\n",
    "UNKNOWN_ELMO = '';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IX-jpTLBrA9R",
    "outputId": "f269e779-97d9-4e3e-b4a0-6e118c54159e"
   },
   "outputs": [],
   "source": [
    "# Obtain Glove Embeddings\n",
    "EMB_DIMS = 300;\n",
    "glove_file = r'glove.6B.{}d.txt'.format(EMB_DIMS)\n",
    "#glove_file = r'glove.840B.{}d.txt'.format(EMB_DIMS)\n",
    "#glove_file = r'glove.42B.{}d.txt'.format(EMB_DIMS)\n",
    "dfg = pd.read_csv(glove_file, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "dfg.loc[UNKNOWN_GLOVE]=[0.0]*EMB_DIMS;\n",
    "print(dfg.shape)\n",
    "\n",
    "# make a hash dict\n",
    "word2id = {};\n",
    "id2word = {};\n",
    "for i, word in tqdm(enumerate(dfg.index.values)):\n",
    "  word2id[word] = i;\n",
    "  id2word[i] = word;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iKkfYSdPHM0"
   },
   "source": [
    "# Rough Work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zu435s6M-E5b"
   },
   "outputs": [],
   "source": [
    "str_ = \"When I lived in Michigan, 1 child at a daycare I believe was around $150.00 per week (divide that by 40 hours and that comes out to about $3.75 and hour). Where I live now, 1 child at a daycare is $80.00 per week ~ which is about $2.00 an hour. I guess the cost depends on the location.11-13-2007, 09:24 AM. memoriesbre. Location: Tunkhannock. 934 posts, read. times. Reputation: 322. I am in Tunkhannock PA and charge $4.00 an hour and take care of infants in my home. am obviously getting the deal of the century here...I pay $3.25/hr per/kid for after care. So that's not that much I guess. I pay my friends daughter (she's 15) once in a while to sit for me and give her $7.00 an hour.\"\n",
    "cleanText(str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rUkAOao-rDUk"
   },
   "outputs": [],
   "source": [
    "# Rough Work\n",
    "#print( np.dot(dfg.loc['u.s.a.'],dfg.loc['u.s.']) )\n",
    "#print( np.dot(dfg.loc['u.s.a.'],dfg.loc['inr']) )\n",
    "dfg.loc['u.s.a.'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwJ6KoTarH_g"
   },
   "outputs": [],
   "source": [
    "re.split('\\W+', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QavYlf9uPPUV"
   },
   "outputs": [],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FNU-AnZcrMXh"
   },
   "outputs": [],
   "source": [
    "re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", 'Hello 2018, the popul@tion is 12,000 per 2000 sq. feet')\n",
    "TextBlob('Hello Merands William AWhile 2018 15mw the sence logik popultion ortunately is 12000 per ur mei 2000 sq feet').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3h-kqCe13PG"
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Notes\n",
    "# Different queries have same query id. Ex: 7977\n",
    "# Some queries donot have right answers. Ex: 280200\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evEnVCATnvOG"
   },
   "source": [
    "# HyperQA Training from Scrath with Glove\n",
    "\n",
    "https://arxiv.org/pdf/1707.07847.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save \n",
    "!mkdir hyperQA_glove_500\n",
    "model_save_folder = r'hyperQA_glove_500'\n",
    "dfile = open('./'+model_save_folder+'/dummy.txt','w',encoding='utf-8');\n",
    "dfile.write('Checking...');\n",
    "dfile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W78o2OWf_WqG"
   },
   "outputs": [],
   "source": [
    "# Utility Funcs\n",
    "def make_glove_batch_data(batch, infer_max_len=False, max_len=0): # batch is list of sentence/paragraph string\n",
    "  arr = [];\n",
    "  b_masks = [];\n",
    "  cstrg_, cstrg_len_ = [], [];\n",
    "  for row in batch:\n",
    "    cstrg__, cstrg_len__ = cleanText(row);\n",
    "    cstrg_.append(cstrg__);\n",
    "    cstrg_len_.append(cstrg_len__);\n",
    "  max_len = np.max(cstrg_len_) if infer_max_len else max_len;\n",
    "  for i, cstrg in enumerate(cstrg_):\n",
    "    if cstrg_len_[i]>max_len:\n",
    "      cstrg = cstrg[:max_len];\n",
    "    elif cstrg_len_[i]<max_len:\n",
    "      cstrg+=[UNKNOWN_GLOVE]*(max_len-cstrg_len_[i])\n",
    "    cstrg = [word if word in word2id.keys() else UNKNOWN_GLOVE for word in cstrg];\n",
    "    bmask = [0.0 if word==UNKNOWN_GLOVE else 1.0 for word in cstrg];\n",
    "    cstrg = [word2id[word] for word in cstrg];\n",
    "    arr.append(cstrg);\n",
    "    b_masks.append(bmask);\n",
    "  return np.vstack(arr), np.vstack(b_masks);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZ8BmSbxKsNb"
   },
   "outputs": [],
   "source": [
    "# HyperQA Tensorflow model\n",
    "\n",
    "class HyperQA(object):\n",
    "    def __init__(self, vocab_embedding, emb_dim, projection_dim, margin, lr=0.001):\n",
    "        # parameters\n",
    "        self.projection_dim = projection_dim;\n",
    "        self.margin = margin;\n",
    "        self.emb_dim = emb_dim;\n",
    "        # placeholder\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.reg_lambda = tf.placeholder(tf.float32, [])\n",
    "        self.query = tf.placeholder(tf.int32, [None, None])\n",
    "        self.query_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.pos_ans = tf.placeholder(tf.int32, [None, None])\n",
    "        self.pos_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.neg_ans = tf.placeholder(tf.int32, [None, None])\n",
    "        self.neg_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        # embedding lookup\n",
    "        self.embedding = tf.Variable(vocab_embedding, trainable=False, dtype=tf.float32, name='embeddings')\n",
    "        #self.embedding = tf.get_variable('embeddings', shape=[400001,self.emb_dim], initializer=tf.contrib.layers.xavier_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.query_emb = tf.nn.embedding_lookup(self.embedding, self.query) # [None,None,emb_dim]\n",
    "        self.pos_ans_emb = tf.nn.embedding_lookup(self.embedding, self.pos_ans) # [None,None,emb_dim]\n",
    "        self.neg_ans_emb = tf.nn.embedding_lookup(self.embedding, self.neg_ans) # [None,None,emb_dim]\n",
    "        # weight vars # a shared variable\n",
    "        self.w_p = tf.get_variable(\"w_p\", shape=[self.emb_dim,self.projection_dim],\n",
    "                                   initializer=tf.contrib.layers.xavier_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.b_p = tf.get_variable('b_p', shape=[self.projection_dim], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.w_f = tf.get_variable(\"w_f\", shape=[1], initializer=tf.contrib.layers.xavier_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.b_f = tf.get_variable('b_f', shape=[1], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "        # projections with input # [None,None,emb_dim] and output # [None,None,proj_emb]\n",
    "        self.query_proj_ = self.project_fn(self.query_emb, self.query_bmask);\n",
    "        self.pos_ans_proj_ = self.project_fn(self.pos_ans_emb, self.pos_ans_bmask);\n",
    "        self.neg_ans_proj_ = self.project_fn(self.neg_ans_emb, self.neg_ans_bmask);\n",
    "        # dropout\n",
    "        self.query_proj = tf.nn.dropout(self.query_proj_, self.keep_prob);\n",
    "        self.pos_ans_proj = tf.nn.dropout(self.pos_ans_proj_, self.keep_prob);\n",
    "        self.neg_ans_proj = tf.nn.dropout(self.neg_ans_proj_, self.keep_prob);\n",
    "        # unit normalized representations with output #[None,proj_emb]\n",
    "        self.query_vec = tf.clip_by_norm(tf.reduce_sum(self.query_proj,axis=1), 1.0, axes=1) \n",
    "        self.pos_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.pos_ans_proj,axis=1), 1.0, axes=1)\n",
    "        self.neg_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.neg_ans_proj,axis=1), 1.0, axes=1)\n",
    "        # hyperbolic distance\n",
    "        self.p_distance = self.hyperbolic_ball(self.query_vec, self.pos_ans_vec) #[None,1]\n",
    "        self.n_distance = self.hyperbolic_ball(self.query_vec, self.neg_ans_vec) #[None,1]\n",
    "        # loss\n",
    "        self.p_score = self.p_distance*self.w_f+self.b_f; \n",
    "        self.n_score = self.n_distance*self.w_f+self.b_f;\n",
    "        self.losses = tf.nn.relu(self.margin + self.n_score - self.p_score) #[None,1]\n",
    "        self.reg_losses = self.reg_lambda*tf.reduce_sum(tf.abs(self.w_p));\n",
    "        self.loss = self.reg_losses+tf.reduce_sum(self.losses) #[]\n",
    "        # print loss ops\n",
    "        self.print_p_distance = tf.reduce_mean(self.p_distance)\n",
    "        self.print_n_distance = tf.reduce_mean(self.n_distance)\n",
    "        self.print_p_score_loss = tf.reduce_mean(self.p_score)\n",
    "        self.print_n_score_loss = tf.reduce_mean(self.n_score)\n",
    "        self.print_losses = tf.reduce_mean(self.losses)\n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        # adjust gradient\n",
    "        gradients = self.optimizer.compute_gradients(self.loss)\n",
    "        reim_gradients = [(self._to_riemannian_gradient(grad), var) for grad, var in gradients]\n",
    "        clip_gradients = [(self._ClipIfNotNone(grad), var) for grad, var in reim_gradients]\n",
    "        self.train_op = self.optimizer.apply_gradients(clip_gradients)       \n",
    "    def project_fn(self, inp_emb, bmask): # Input Shape [None,None,emb_dim], [None,None]\n",
    "        runtime_shape = tf.shape(inp_emb);\n",
    "        dim1 = runtime_shape[0];\n",
    "        dim2 = runtime_shape[1];\n",
    "        dense_output = tf.nn.xw_plus_b(tf.reshape(inp_emb, [-1,self.emb_dim]), self.w_p, self.b_p);\n",
    "        activated_output = tf.nn.relu(dense_output);\n",
    "        proj_emb = tf.reshape(activated_output,[dim1,dim2,self.projection_dim]);\n",
    "        bmask = tf.tile(tf.expand_dims(bmask,axis=-1),[1,1,self.projection_dim]);\n",
    "        masked_proj_emb = bmask*proj_emb;\n",
    "        return masked_proj_emb\n",
    "    def hyperbolic_ball(self, x, y, neg=False, eps=1E-6):\n",
    "        \"\"\" Poincare Distance Function \"\"\"\n",
    "        z = x - y\n",
    "        z = tf.norm(z, ord='euclidean', keep_dims=True, axis=1)\n",
    "        z = tf.square(z)\n",
    "        x_d = 1 - tf.square(tf.norm(x, ord='euclidean', keep_dims=True, axis=1))\n",
    "        y_d = 1 - tf.square(tf.norm(y, ord='euclidean', keep_dims=True, axis=1))\n",
    "        d = x_d * y_d\n",
    "        z = z / (d + eps)\n",
    "        z  = (2 * z) + 1\n",
    "        arcosh = z + tf.sqrt(tf.square(z) - 1 + eps)\n",
    "        arcosh = tf.log(arcosh)\n",
    "        if(neg):\n",
    "            arcosh = -arcosh\n",
    "        return arcosh\n",
    "    def _ClipIfNotNone(self, grad):\n",
    "        if grad is None:\n",
    "          return grad\n",
    "        grad = tf.clip_by_value(grad, -10, 10, name=None)\n",
    "        #grad = tf.clip_by_norm(grad, 1.0)\n",
    "        return grad\n",
    "    def _to_riemannian_gradient(self, ge):\n",
    "      if ge is None:\n",
    "        return None\n",
    "      try:\n",
    "        shape = ge.get_shape().as_list()\n",
    "        if len(shape) >= 3:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, axis=[-2, -1], keepdims=True))\n",
    "        elif len(shape) == 2:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, keepdims=True))\n",
    "        else:\n",
    "            return ge\n",
    "      except:\n",
    "        print('Exception handled!')\n",
    "        grad_scale = 1 - tf.square(tf.norm(ge, keep_dims=True))\n",
    "      grad_scale = (tf.square(grad_scale) + 1e-10) / 4.0\n",
    "      gr = ge * grad_scale\n",
    "      # gr = tf.clip_by_norm(gr, 1.0, axes=0)\n",
    "      return gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZpiKMwlFeHH"
   },
   "outputs": [],
   "source": [
    "# some global vars\n",
    "PROJ_DIMS = 500;\n",
    "\n",
    "# args\n",
    "MARGIN = 5.0;\n",
    "LEARNING_RATE = 0.001;\n",
    "TRAINING_BATCH_SIZE = 128;\n",
    "VALIDATION_BATCH_SIZE = 1024;\n",
    "VALIDATION_EVERY = 1;\n",
    "N_EPOCHS = 100;\n",
    "\n",
    "# Create a graph with given params\n",
    "tf.reset_default_graph();\n",
    "hyperQA_Graph = tf.Graph();\n",
    "with hyperQA_Graph.as_default():\n",
    "  hyperQA = HyperQA(dfg.as_matrix(), EMB_DIMS, PROJ_DIMS, MARGIN, LEARNING_RATE)\n",
    "  print('graph built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWRs1_9HtVxX"
   },
   "outputs": [],
   "source": [
    "# NEW: max sampling from https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_CIKM2016.pdf\n",
    "# Run this cell for both random_ as well as max_sampling\n",
    "choice_max = 0.75;\n",
    "default_sampling_choice = 'RANDOM';\n",
    "RESUME_TRAINING = False;\n",
    "START_EPOCH = 0;\n",
    "\n",
    "# Run the connections for desired epochs\n",
    "with tf.Session(graph=hyperQA_Graph, config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  if not RESUME_TRAINING:\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    START_EPOCH = 0;\n",
    "  else:\n",
    "    saver.restore(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(START_EPOCH-1));\n",
    "  for epoch in np.arange(START_EPOCH, N_EPOCHS):\n",
    "    # sampling choice\n",
    "    if epoch==0:\n",
    "      sampling_choice = default_sampling_choice;\n",
    "    else:\n",
    "      sampling_choice = 'MAX' if np.random.random()<choice_max else default_sampling_choice;\n",
    "    # Loss and optimization on Training Data\n",
    "    print('beginning epoch: {}'.format(epoch));\n",
    "    n_batches = int(np.floor(n_train/TRAINING_BATCH_SIZE));\n",
    "    cum_loss = 0;\n",
    "    cum_pos_loss = 0;\n",
    "    cum_neg_loss = 0;\n",
    "    frm=0;\n",
    "    # TRAIN\n",
    "    for i in range(n_batches):\n",
    "      batch_query_ids = training_query_ids[frm:frm+TRAINING_BATCH_SIZE];\n",
    "      batch_queries = [];\n",
    "      batch_pos = [];\n",
    "      batch_neg = [];\n",
    "      batch_arg_neg = [];\n",
    "      # TRAIN: Find the most challenging negative!\n",
    "      if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "        dummy_batch_queries = [];\n",
    "        dummy_batch_neg = [];\n",
    "        dummy_batch_neg_n = [];\n",
    "        for query_id in batch_query_ids:\n",
    "          dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "          for jj in data_dict[query_id]['negs']:\n",
    "            dummy_batch_neg.append(id2passage[jj]);\n",
    "          dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))\n",
    "        d_x, d_x1 = make_glove_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "        d_z, d_z1 = make_glove_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "        d_x_new, d_x1_new = [], [];\n",
    "        for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "          d_x_new.append(np.tile(d_x[ii],(n_times,1)));\n",
    "          d_x1_new.append(np.tile(d_x1[ii],(n_times,1)));\n",
    "        d_x_new, d_x1_new = np.vstack(d_x_new), np.vstack(d_x1_new);\n",
    "        result = sess.run([hyperQA.n_score],feed_dict={hyperQA.keep_prob: 1.0,\n",
    "                                                       hyperQA.query:d_x_new,\n",
    "                                                       hyperQA.query_bmask:d_x1_new,\n",
    "                                                       hyperQA.neg_ans:d_z,\n",
    "                                                       hyperQA.neg_ans_bmask:d_z1})\n",
    "        result = result[0]; # array of batch size i.e. (BS,1)\n",
    "        k=0;\n",
    "        for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "          batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "          k+=n_times;\n",
    "      # TRAIN: compute loss and optimize\n",
    "      if sampling_choice=='MAX':\n",
    "        assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "      for i_id, query_id in enumerate(batch_query_ids):\n",
    "        batch_queries.append(data_dict[query_id]['query']);\n",
    "        batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "        this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "        batch_neg.append(this_negative);\n",
    "      x, x1 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "      y, y1 = make_glove_batch_data(batch_pos,infer_max_len=True) \n",
    "      z, z1 = make_glove_batch_data(batch_neg,infer_max_len=True)\n",
    "      result = sess.run([hyperQA.train_op, hyperQA.print_losses, hyperQA.print_p_score_loss, hyperQA.print_n_score_loss],\n",
    "                        feed_dict={hyperQA.keep_prob: 0.8,\n",
    "                                   hyperQA.reg_lambda: 0.00001,\n",
    "                                   hyperQA.query:x,\n",
    "                                   hyperQA.query_bmask:x1,\n",
    "                                   hyperQA.pos_ans:y,\n",
    "                                   hyperQA.pos_ans_bmask:y1,\n",
    "                                   hyperQA.neg_ans:z,\n",
    "                                   hyperQA.neg_ans_bmask:z1})\n",
    "      cum_loss+=result[1];\n",
    "      cum_pos_loss+=result[2];\n",
    "      cum_neg_loss+=result[3];\n",
    "      frm+=TRAINING_BATCH_SIZE;\n",
    "      progressBar(frm,n_train,['loss','pos_loss','neg_loss'],[result[1],result[2],result[3]]);\n",
    "    print('\\nTRAINING || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))\n",
    "    saver.save(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(epoch));\n",
    "    # VALIDATE\n",
    "    if epoch%VALIDATION_EVERY==0:\n",
    "      #saver = tf.train.Saver();\n",
    "      #saver.restore(val_sess, './'+model_save_folder+'/epoch_model_20.ckpt');\n",
    "      n_batches = int(np.floor(n_val/VALIDATION_BATCH_SIZE));\n",
    "      cum_loss = 0;\n",
    "      cum_pos_loss = 0;\n",
    "      cum_neg_loss = 0;\n",
    "      frm=0;\n",
    "      for i in range(n_batches):\n",
    "        batch_query_ids = validation_query_ids[frm:frm+VALIDATION_BATCH_SIZE];\n",
    "        batch_queries = [];\n",
    "        batch_pos = [];\n",
    "        batch_neg = [];\n",
    "        batch_arg_neg = [];\n",
    "        # VALIDATE: Find the most challenging negative!\n",
    "        if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "          dummy_batch_queries = [];\n",
    "          dummy_batch_neg = [];\n",
    "          dummy_batch_neg_n = [];\n",
    "          for query_id in batch_query_ids:\n",
    "            dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "            for jj in data_dict[query_id]['negs']:\n",
    "              dummy_batch_neg.append(id2passage[jj]);\n",
    "            dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))\n",
    "          d_x, d_x1 = make_glove_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "          d_z, d_z1 = make_glove_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "          d_x_new, d_x1_new = [], [];\n",
    "          for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "            d_x_new.append(np.tile(d_x[ii],(n_times,1)));\n",
    "            d_x1_new.append(np.tile(d_x1[ii],(n_times,1)));\n",
    "          d_x_new, d_x1_new = np.vstack(d_x_new), np.vstack(d_x1_new);\n",
    "          result = sess.run([hyperQA.n_score],feed_dict={hyperQA.keep_prob: 1.0,\n",
    "                                                         hyperQA.query:d_x_new,\n",
    "                                                         hyperQA.query_bmask:d_x1_new,\n",
    "                                                         hyperQA.neg_ans:d_z,\n",
    "                                                         hyperQA.neg_ans_bmask:d_z1})\n",
    "          result = result[0]; # array of batch size i.e. (BS,1)\n",
    "          k=0;\n",
    "          for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "            batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "            k+=n_times;\n",
    "        # VALIDATE: compute loss only\n",
    "        if sampling_choice=='MAX':\n",
    "          assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "        for i_id, query_id in enumerate(batch_query_ids):\n",
    "          batch_queries.append(data_dict[query_id]['query']);\n",
    "          batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "          this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "          batch_neg.append(this_negative);\n",
    "        x, x1 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "        y, y1 = make_glove_batch_data(batch_pos,infer_max_len=True) \n",
    "        z, z1 = make_glove_batch_data(batch_neg,infer_max_len=True)\n",
    "        result = sess.run([hyperQA.print_losses, hyperQA.print_p_score_loss, hyperQA.print_n_score_loss],\n",
    "                          feed_dict={hyperQA.keep_prob:1.0,\n",
    "                                     hyperQA.reg_lambda:0.0,\n",
    "                                     hyperQA.query:x,\n",
    "                                     hyperQA.query_bmask:x1,\n",
    "                                     hyperQA.pos_ans:y,\n",
    "                                     hyperQA.pos_ans_bmask:y1,\n",
    "                                     hyperQA.neg_ans:z,\n",
    "                                     hyperQA.neg_ans_bmask:z1})\n",
    "        cum_loss+=result[0];\n",
    "        cum_pos_loss+=result[1];\n",
    "        cum_neg_loss+=result[2];\n",
    "        frm+=VALIDATION_BATCH_SIZE;\n",
    "        progressBar(frm,n_val,['loss','pos_loss','neg_loss'],[result[0],result[1],result[2]]);\n",
    "      print('\\nVALIDATION || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m4Z4KCIOs7Oe"
   },
   "outputs": [],
   "source": [
    "# args\n",
    "EVAL_BATCH_SIZE = 1024;\n",
    "\n",
    "# load data\n",
    "df_test = pd.read_csv(\"eval1_unlabelled.tsv\", sep= '\\t', header=None)\n",
    "print('Eval Data Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGrf60Up1P1O"
   },
   "outputs": [],
   "source": [
    "# get scores\n",
    "outputs = [];\n",
    "with tf.Session(graph=hyperQA_Graph, config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  saver.restore(sess, r'./'+model_save_folder+'/epoch_model_20.ckpt');\n",
    "  for i, row in df_test.iterrows():\n",
    "    if i==0:\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "      batch_ql_max = -1;\n",
    "      batch_pl_max = -1;\n",
    "    if i!=0 and i%EVAL_BATCH_SIZE==0:\n",
    "      x, x1 = make_glove_batch_data(batch_queries,infer_max_len=True);\n",
    "      y, y1 = make_glove_batch_data(batch_passages,infer_max_len=True);\n",
    "      result = sess.run([hyperQA.p_score],feed_dict={hyperQA.keep_prob: 1.0,\n",
    "                                                     hyperQA.query:x,\n",
    "                                                     hyperQA.query_bmask:x1,\n",
    "                                                     hyperQA.pos_ans:y,\n",
    "                                                     hyperQA.pos_ans_bmask:y1})\n",
    "      outputs.append(result[0].tolist());\n",
    "      progressBar(i,df_test.shape[0]);\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "      batch_ql_max = -1;\n",
    "      batch_pl_max = -1;\n",
    "    query, passage = row[1], row[2];\n",
    "    batch_queries.append(query);\n",
    "    batch_passages.append(passage);\n",
    "  if batch_queries:\n",
    "    x, x1 = make_glove_batch_data(batch_queries,infer_max_len=True);\n",
    "    y, y1 = make_glove_batch_data(batch_passages,infer_max_len=True);\n",
    "    result = sess.run([hyperQA.p_score],feed_dict={hyperQA.keep_prob: 1.0,\n",
    "                                                   hyperQA.query:x,\n",
    "                                                   hyperQA.query_bmask:x1,\n",
    "                                                   hyperQA.pos_ans:y,\n",
    "                                                   hyperQA.pos_ans_bmask:y1})\n",
    "    outputs.append(result[0].tolist());\n",
    "    progressBar(i,df_test.shape[0]);\n",
    "    batch_queries = [];\n",
    "    batch_passages = [];\n",
    "    batch_ql_max = -1;\n",
    "    batch_pl_max = -1;\n",
    "    \n",
    "outputs = np.vstack(outputs)\n",
    "df_test['pred'] = outputs; # the smaller the score, the more positive_answer it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kivt4Ump1ZUL"
   },
   "outputs": [],
   "source": [
    "# re-using code from Dileep\n",
    "outfilename = 'answer.tsv'\n",
    "with open(outfilename,\"w\",encoding=\"utf-8\") as fw:\n",
    "  import math\n",
    "  linelist = []\n",
    "  tempscores = []\n",
    "  for idx, row in tqdm(df_test.iterrows()):\n",
    "      tempscores.append(row['pred'])\n",
    "      #tempscores.append(str(row['pred']))\n",
    "      if((idx +1)%10==0):\n",
    "          tempscores-=np.min(tempscores);\n",
    "          tempscores = [math.exp(s) for s in tempscores];\n",
    "          expsum = sum(tempscores)\n",
    "          tempscores = [str(s/expsum) for s in tempscores]\n",
    "          scoreString = \"\\t\".join(tempscores)\n",
    "          qid = str(row[0])\n",
    "          fw.write(qid+\"\\t\"+scoreString+\"\\n\")\n",
    "          tempscores=[]\n",
    "      #if(idx%10000==0):\n",
    "      #    print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8T70ZlM4LTnx"
   },
   "source": [
    "# HyperQA Training from Scrath with Glove and ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save \n",
    "!mkdir hyperQA_glove_elmo_100\n",
    "model_save_folder = r'hyperQA_glove_elmo_100'\n",
    "dfile = open('./'+model_save_folder+'/dummy.txt','w',encoding='utf-8');\n",
    "dfile.write('Checking...');\n",
    "dfile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_glove_and_elmo_batch_data(batch, infer_max_len=True, max_len=0): # batch is list of sentence/paragraph string\n",
    "  glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = [], [], [], [], [];\n",
    "  cstrg_, cstrg_len_ = [], [];\n",
    "  for row in batch:\n",
    "    cstrg__, cstrg_len__ = cleanText(row);\n",
    "    cstrg_.append(cstrg__);\n",
    "    cstrg_len_.append(cstrg_len__);\n",
    "  max_len = np.max(cstrg_len_) if infer_max_len else max_len;\n",
    "  for i, cstrg in enumerate(cstrg_):\n",
    "    cstrg_glove, cstrg_elmo = [], [];    \n",
    "    if cstrg_len_[i]>=max_len:\n",
    "      cstrg_glove, cstrg_elmo = cstrg[:max_len].copy(), cstrg[:max_len].copy();\n",
    "      n_tokens.append(max_len);\n",
    "    elif cstrg_len_[i]<max_len:\n",
    "      cstrg_glove, cstrg_elmo = cstrg.copy(), cstrg.copy(); \n",
    "      cstrg_glove+=[UNKNOWN_GLOVE]*(max_len-cstrg_len_[i])\n",
    "      cstrg_elmo+=[UNKNOWN_ELMO]*(max_len-cstrg_len_[i])\n",
    "      n_tokens.append(cstrg_len_[i]);\n",
    "    # as per glove requirement\n",
    "    cstrg_glove = [word if word in word2id.keys() else UNKNOWN_GLOVE for word in cstrg_glove];\n",
    "    bmask_glove = [0.0 if word==UNKNOWN_GLOVE else 1.0 for word in cstrg_glove];\n",
    "    cstrg_glove = [word2id[word] for word in cstrg_glove];\n",
    "    glove_arr.append(cstrg_glove);\n",
    "    glove_bmasks.append(bmask_glove);\n",
    "    # as per elmo requirement\n",
    "    elmo_arr.append(cstrg_elmo);\n",
    "    bmask_elmo = [0.0 if word==UNKNOWN_ELMO else 1.0 for word in cstrg_elmo];\n",
    "    elmo_bmasks.append(bmask_elmo);\n",
    "  return glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens\n",
    "  #return np.vstack(glove_arr), np.vstack(glove_bmasks), np.vstack(elmo_arr), np.vstack(elmo_bmasks), n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNWGQgpj3AgS"
   },
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb\n",
    "# https://tfhub.dev/google/elmo/2\n",
    "# https://tfhub.dev/google/universal-sentence-encoder/2\n",
    "# https://github.com/PrashantRanjan09/WordEmbeddings-Elmo-Fasttext-Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8mmwRu-1kN7"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ag_zX_gO1p-V"
   },
   "outputs": [],
   "source": [
    "# HyperQA Tensorflow model\n",
    "\n",
    "class HyperQA_ELMo(object):\n",
    "    def __init__(self, vocab_embedding, glove_emb_dim, projection_dim, margin, lr=0.001, elmo=None, elmo_emb_dim=1024):\n",
    "\n",
    "        # parameters and variables\n",
    "        self.glove_embedding = tf.Variable(vocab_embedding, trainable=False, dtype=tf.float32, name='glove_embedding')\n",
    "        self.glove_emb_dim = glove_emb_dim;\n",
    "        self.projection_dim = projection_dim;\n",
    "        self.margin = margin;\n",
    "        self.lr = lr;\n",
    "        self.elmo = elmo;\n",
    "        self.elmo_emb_dim = elmo_emb_dim;\n",
    "        self.concat_dim = self.glove_emb_dim+self.elmo_emb_dim;\n",
    "        self.init_name = tf.contrib.layers.xavier_initializer()\n",
    "        self.w_p = tf.get_variable(\"w_p\", shape=[self.concat_dim,self.projection_dim], initializer=self.init_name, trainable=True, dtype=tf.float32)\n",
    "        self.b_p = tf.get_variable('b_p', shape=[self.projection_dim], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.w_f = tf.get_variable(\"w_f\", shape=[1], initializer=self.init_name, trainable=True, dtype=tf.float32)\n",
    "        self.b_f = tf.get_variable('b_f', shape=[1], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "\n",
    "        # placeholder: general\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.reg_lambda = tf.placeholder(tf.float32, [])\n",
    "        # placeholder: glove\n",
    "        self.query_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        self.pos_ans_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        self.neg_ans_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        # placeholder: elmo\n",
    "        self.query_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.query_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.pos_ans_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.pos_ans_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.neg_ans_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.neg_ans_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        # placeholder: masking\n",
    "        self.query_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.pos_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.neg_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "\n",
    "\n",
    "        # get glove embedding # [None,None,glove_emb_dim]\n",
    "        self.query_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.query_glove)\n",
    "        self.pos_ans_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.pos_ans_glove)\n",
    "        self.neg_ans_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.neg_ans_glove) \n",
    "\n",
    "        # get elmo embeddings # [None,None,elmo_emb_dim]\n",
    "        self.query_elmo_emb = self.ElmoEmbedding(self.query_elmo, self.query_elmo_tokens);\n",
    "        self.pos_ans_elmo_emb = self.ElmoEmbedding(self.pos_ans_elmo, self.pos_ans_elmo_tokens);\n",
    "        self.neg_ans_elmo_emb = self.ElmoEmbedding(self.neg_ans_elmo, self.neg_ans_elmo_tokens);\n",
    "\n",
    "        # concat embeddings # [None,None,concat_dim]\n",
    "        self.query_emb = tf.concat([self.query_glove_emb,self.query_elmo_emb],axis=-1)\n",
    "        self.pos_ans_emb = tf.concat([self.pos_ans_glove_emb,self.pos_ans_elmo_emb],axis=-1)\n",
    "        self.neg_ans_emb = tf.concat([self.neg_ans_glove_emb,self.neg_ans_elmo_emb],axis=-1)\n",
    "\n",
    "        # projections with input # [None,None,glove_emb_dim] and output # [None,None,proj_emb]\n",
    "        self.query_proj_ = self.project_fn(self.query_emb, self.query_bmask);\n",
    "        self.pos_ans_proj_ = self.project_fn(self.pos_ans_emb, self.pos_ans_bmask);\n",
    "        self.neg_ans_proj_ = self.project_fn(self.neg_ans_emb, self.neg_ans_bmask);\n",
    "        # dropout\n",
    "        self.query_proj = tf.nn.dropout(self.query_proj_, self.keep_prob);\n",
    "        self.pos_ans_proj = tf.nn.dropout(self.pos_ans_proj_, self.keep_prob);\n",
    "        self.neg_ans_proj = tf.nn.dropout(self.neg_ans_proj_, self.keep_prob);\n",
    "        # unit normalized representations with output #[None,proj_emb]\n",
    "        self.query_vec = tf.clip_by_norm(tf.reduce_sum(self.query_proj,axis=1), 1.0, axes=1) \n",
    "        self.pos_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.pos_ans_proj,axis=1), 1.0, axes=1)\n",
    "        self.neg_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.neg_ans_proj,axis=1), 1.0, axes=1)\n",
    "        # hyperbolic distance\n",
    "        self.p_distance = self.hyperbolic_ball(self.query_vec, self.pos_ans_vec) #[None,1]\n",
    "        self.n_distance = self.hyperbolic_ball(self.query_vec, self.neg_ans_vec) #[None,1]\n",
    "        # loss\n",
    "        self.p_score = self.p_distance*self.w_f+self.b_f; \n",
    "        self.n_score = self.n_distance*self.w_f+self.b_f;\n",
    "        self.losses = tf.nn.relu(self.margin + self.n_score - self.p_score) #[None,1]\n",
    "        self.reg_losses = self.reg_lambda*tf.reduce_sum(tf.abs(self.w_p));\n",
    "        self.loss = self.reg_losses+tf.reduce_sum(self.losses) #[]\n",
    "        # print loss ops\n",
    "        self.print_p_distance = tf.reduce_mean(self.p_distance)\n",
    "        self.print_n_distance = tf.reduce_mean(self.n_distance)\n",
    "        self.print_p_score_loss = tf.reduce_mean(self.p_score)\n",
    "        self.print_n_score_loss = tf.reduce_mean(self.n_score)\n",
    "        self.print_losses = tf.reduce_mean(self.losses)\n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        # adjust gradient\n",
    "        gradients = self.optimizer.compute_gradients(self.loss)\n",
    "        reim_gradients = [(self._to_riemannian_gradient(grad), var) for grad, var in gradients]\n",
    "        clip_gradients = [(self._ClipIfNotNone(grad), var) for grad, var in reim_gradients]\n",
    "        self.train_op = self.optimizer.apply_gradients(clip_gradients)  \n",
    "\n",
    "    def ElmoEmbedding(self, tokens_input, tokens_length): #inputs here are tensors \n",
    "        return self.elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature=\"tokens\",as_dict=True)[\"elmo\"]     \n",
    "    def project_fn(self, inp_emb, bmask): # Input Shape [None,None,concat_dim], [None,None]\n",
    "        runtime_shape = tf.shape(inp_emb);\n",
    "        dim1 = runtime_shape[0];\n",
    "        dim2 = runtime_shape[1];\n",
    "        dense_output = tf.nn.xw_plus_b(tf.reshape(inp_emb, [-1,self.concat_dim]), self.w_p, self.b_p);\n",
    "        activated_output = tf.nn.relu(dense_output);\n",
    "        proj_emb = tf.reshape(activated_output,[dim1,dim2,self.projection_dim]);\n",
    "        bmask = tf.tile(tf.expand_dims(bmask,axis=-1),[1,1,self.projection_dim]);\n",
    "        masked_proj_emb = bmask*proj_emb;\n",
    "        return masked_proj_emb\n",
    "    def hyperbolic_ball(self, x, y, neg=False, eps=1E-6):\n",
    "        \"\"\" Poincare Distance Function \"\"\"\n",
    "        z = x - y\n",
    "        z = tf.norm(z, ord='euclidean', keep_dims=True, axis=1)\n",
    "        z = tf.square(z)\n",
    "        x_d = 1 - tf.square(tf.norm(x, ord='euclidean', keep_dims=True, axis=1))\n",
    "        y_d = 1 - tf.square(tf.norm(y, ord='euclidean', keep_dims=True, axis=1))\n",
    "        d = x_d * y_d\n",
    "        z = z / (d + eps)\n",
    "        z  = (2 * z) + 1\n",
    "        arcosh = z + tf.sqrt(tf.square(z) - 1 + eps)\n",
    "        arcosh = tf.log(arcosh)\n",
    "        if(neg):\n",
    "            arcosh = -arcosh\n",
    "        return arcosh\n",
    "    def _ClipIfNotNone(self, grad):\n",
    "        if grad is None:\n",
    "          return grad\n",
    "        grad = tf.clip_by_value(grad, -10, 10, name=None)\n",
    "        #grad = tf.clip_by_norm(grad, 1.0)\n",
    "        return grad\n",
    "    def _to_riemannian_gradient(self, ge):\n",
    "      if ge is None:\n",
    "        return None\n",
    "      try:\n",
    "        shape = ge.get_shape().as_list()\n",
    "        if len(shape) >= 3:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, axis=[-2, -1], keepdims=True))\n",
    "        elif len(shape) == 2:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, keepdims=True))\n",
    "        else:\n",
    "            return ge\n",
    "      except:\n",
    "        print('Exception handled!')\n",
    "        grad_scale = 1 - tf.square(tf.norm(ge, keep_dims=True))\n",
    "      grad_scale = (tf.square(grad_scale) + 1e-10) / 4.0\n",
    "      gr = ge * grad_scale\n",
    "      # gr = tf.clip_by_norm(gr, 1.0, axes=0)\n",
    "      return gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pynZt8KDBA4y"
   },
   "outputs": [],
   "source": [
    "# some global vars\n",
    "PROJ_DIMS = 100;\n",
    "MARGIN = 5.0;\n",
    "LEARNING_RATE = 0.001;\n",
    "TRAINING_BATCH_SIZE = 32;\n",
    "VALIDATION_BATCH_SIZE = 32;\n",
    "VALIDATION_EVERY = 1;\n",
    "N_EPOCHS = 100;\n",
    "\n",
    "# Create a graph with given params\n",
    "tf.reset_default_graph();\n",
    "elmo = hub.Module(\"elmo2\", trainable=True)\n",
    "hyperQA_ELMo = HyperQA_ELMo(dfg.values, EMB_DIMS, PROJ_DIMS, MARGIN, LEARNING_RATE, elmo=elmo, elmo_emb_dim=1024)\n",
    "print('graph built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMwH-tKh1sdn"
   },
   "outputs": [],
   "source": [
    "# NEW: max sampling from https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_CIKM2016.pdf\n",
    "# Run this cell for both random_ as well as max_sampling\n",
    "choice_max = 0; # the greater this is, the more cprob to chosse MAX over RANDOM\n",
    "default_sampling_choice = 'RANDOM';\n",
    "RESUME_TRAINING = True;\n",
    "START_EPOCH = 1;\n",
    "\n",
    "# Run the connections for desired epochs\n",
    "with tf.Session(config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  if not RESUME_TRAINING:\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    START_EPOCH = 0;\n",
    "  else:\n",
    "    saver.restore(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(START_EPOCH-1));\n",
    "  for epoch in np.arange(START_EPOCH, N_EPOCHS):\n",
    "    # sampling choice\n",
    "    if epoch==0:\n",
    "      sampling_choice = default_sampling_choice;\n",
    "    else:\n",
    "      sampling_choice = 'MAX' if np.random.random()<choice_max else default_sampling_choice;\n",
    "    # Loss and optimization on Training Data\n",
    "    print('beginning epoch: {}'.format(epoch));\n",
    "    n_batches = int(np.floor(n_train/TRAINING_BATCH_SIZE));\n",
    "    cum_loss = 0;\n",
    "    cum_pos_loss = 0;\n",
    "    cum_neg_loss = 0;\n",
    "    frm=0;\n",
    "    # TRAIN\n",
    "    for i in range(n_batches):\n",
    "      if i%1000==0:\n",
    "        saver.save(sess, './'+model_save_folder+'/batch_model.ckpt'.format(epoch));\n",
    "      batch_query_ids = training_query_ids[frm:frm+TRAINING_BATCH_SIZE];\n",
    "      batch_queries = [];\n",
    "      batch_pos = [];\n",
    "      batch_neg = [];\n",
    "      batch_arg_neg = [];\n",
    "      # TRAIN: Find the most challenging negative!\n",
    "      if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "        unfort_index = 0;\n",
    "        while(unfort_index<len(batch_query_ids)):\n",
    "            small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "            dummy_batch_queries = [];\n",
    "            dummy_batch_neg = [];\n",
    "            dummy_batch_neg_n = [];\n",
    "            for query_id in small_batch_query_ids:\n",
    "              dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "              for jj in data_dict[query_id]['negs']:\n",
    "                dummy_batch_neg.append(id2passage[jj]);\n",
    "              dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))        \n",
    "            #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "            aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "            bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "            aa1_new, aa3_new, aa4_new, aa5_new = [], [], [], [];\n",
    "            for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "              aa1_new.append(np.tile(aa1[ii],(n_times,1)));\n",
    "              aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "              aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "              aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "            aa1_new, aa3_new, aa4_new, aa5_new = np.vstack(aa1_new), np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "            aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "            result = sess.run([hyperQA_ELMo.n_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                                hyperQA_ELMo.query_glove:aa1_new,\n",
    "                                                                hyperQA_ELMo.query_elmo:aa3_new,\n",
    "                                                                hyperQA_ELMo.query_bmask:aa4_new,\n",
    "                                                                hyperQA_ELMo.query_elmo_tokens:aa5_new,\n",
    "                                                                hyperQA_ELMo.neg_ans_glove:bb1,\n",
    "                                                                hyperQA_ELMo.neg_ans_elmo:bb3,\n",
    "                                                                hyperQA_ELMo.neg_ans_bmask:bb4,\n",
    "                                                                hyperQA_ELMo.neg_ans_elmo_tokens:bb5})\n",
    "            result = result[0]; # array of batch size i.e. (BS,1)\n",
    "            k=0;\n",
    "            for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "              batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "              k+=n_times;\n",
    "            unfort_index+=3;\n",
    "      # TRAIN: compute loss and optimize\n",
    "      if sampling_choice=='MAX':\n",
    "        assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "      for i_id, query_id in enumerate(batch_query_ids):\n",
    "        batch_queries.append(data_dict[query_id]['query']);\n",
    "        batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "        this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "        batch_neg.append(this_negative);\n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(batch_pos,infer_max_len=True) \n",
    "      cc1, _, cc3, cc4, cc5 = make_glove_and_elmo_batch_data(batch_neg,infer_max_len=True)\n",
    "      result = sess.run([hyperQA_ELMo.train_op, hyperQA_ELMo.print_losses, hyperQA_ELMo.print_p_score_loss, hyperQA_ELMo.print_n_score_loss],\n",
    "                        feed_dict={hyperQA_ELMo.keep_prob: 0.8,\n",
    "                                   hyperQA_ELMo.reg_lambda: 0.00001,\n",
    "                                   hyperQA_ELMo.query_glove:aa1,\n",
    "                                   hyperQA_ELMo.pos_ans_glove:bb1,\n",
    "                                   hyperQA_ELMo.neg_ans_glove:cc1,\n",
    "                                   hyperQA_ELMo.query_elmo:aa3,\n",
    "                                   hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                   hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                   hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                   hyperQA_ELMo.neg_ans_elmo:cc3,\n",
    "                                   hyperQA_ELMo.neg_ans_elmo_tokens:cc5,\n",
    "                                   hyperQA_ELMo.query_bmask:aa4,\n",
    "                                   hyperQA_ELMo.pos_ans_bmask:bb4,\n",
    "                                   hyperQA_ELMo.neg_ans_bmask:cc4})    \n",
    "      cum_loss+=result[1];\n",
    "      cum_pos_loss+=result[2];\n",
    "      cum_neg_loss+=result[3];\n",
    "      frm+=TRAINING_BATCH_SIZE;\n",
    "      progressBar(frm,n_train,['loss','pos_loss','neg_loss'],[result[1],result[2],result[3]]);\n",
    "    print('\\nTRAINING || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))\n",
    "    saver.save(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(epoch));\n",
    "    # VALIDATE\n",
    "    if epoch!=0 and epoch%VALIDATION_EVERY==0:\n",
    "      #saver = tf.train.Saver();\n",
    "      #saver.restore(val_sess, './'+model_save_folder+'/epoch_model_20.ckpt');\n",
    "      n_batches = int(np.floor(n_val/VALIDATION_BATCH_SIZE));\n",
    "      cum_loss = 0;\n",
    "      cum_pos_loss = 0;\n",
    "      cum_neg_loss = 0;\n",
    "      frm=0;\n",
    "      for i in range(n_batches):\n",
    "        batch_query_ids = validation_query_ids[frm:frm+VALIDATION_BATCH_SIZE];\n",
    "        batch_queries = [];\n",
    "        batch_pos = [];\n",
    "        batch_neg = [];\n",
    "        batch_arg_neg = [];\n",
    "        # VALIDATE: Find the most challenging negative!\n",
    "        if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "            unfort_index = 0;\n",
    "            while(unfort_index<len(batch_query_ids)):\n",
    "                small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "                dummy_batch_queries = [];\n",
    "                dummy_batch_neg = [];\n",
    "                dummy_batch_neg_n = [];\n",
    "                for query_id in small_batch_query_ids:\n",
    "                  dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "                  for jj in data_dict[query_id]['negs']:\n",
    "                    dummy_batch_neg.append(id2passage[jj]);\n",
    "                  dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))        \n",
    "                #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "                aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "                bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "                aa1_new, aa3_new, aa4_new, aa5_new = [], [], [], [];\n",
    "                for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "                  aa1_new.append(np.tile(aa1[ii],(n_times,1)));\n",
    "                  aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "                  aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "                  aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "                aa1_new, aa3_new, aa4_new, aa5_new = np.vstack(aa1_new), np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "                aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "                result = sess.run([hyperQA_ELMo.n_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                                    hyperQA_ELMo.query_glove:aa1_new,\n",
    "                                                                    hyperQA_ELMo.query_elmo:aa3_new,\n",
    "                                                                    hyperQA_ELMo.query_bmask:aa4_new,\n",
    "                                                                    hyperQA_ELMo.query_elmo_tokens:aa5_new,\n",
    "                                                                    hyperQA_ELMo.neg_ans_glove:bb1,\n",
    "                                                                    hyperQA_ELMo.neg_ans_elmo:bb3,\n",
    "                                                                    hyperQA_ELMo.neg_ans_bmask:bb4,\n",
    "                                                                    hyperQA_ELMo.neg_ans_elmo_tokens:bb5})\n",
    "                result = result[0]; # array of batch size i.e. (BS,1)\n",
    "                k=0;\n",
    "                for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "                  batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "                  k+=n_times;\n",
    "                unfort_index+=3;\n",
    "        # VALIDATE: compute loss only\n",
    "        if sampling_choice=='MAX':\n",
    "          assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "        for i_id, query_id in enumerate(batch_query_ids):\n",
    "          batch_queries.append(data_dict[query_id]['query']);\n",
    "          batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "          this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "          batch_neg.append(this_negative);\n",
    "        #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "        aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "        bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(batch_pos,infer_max_len=True) \n",
    "        cc1, _, cc3, cc4, cc5 = make_glove_and_elmo_batch_data(batch_neg,infer_max_len=True)\n",
    "        result = sess.run([hyperQA_ELMo.print_losses, hyperQA_ELMo.print_p_score_loss, hyperQA_ELMo.print_n_score_loss],\n",
    "                          feed_dict={hyperQA_ELMo.keep_prob: 0.8,\n",
    "                                     hyperQA_ELMo.reg_lambda: 0.00001,\n",
    "                                     hyperQA_ELMo.query_glove:aa1,\n",
    "                                     hyperQA_ELMo.pos_ans_glove:bb1,\n",
    "                                     hyperQA_ELMo.neg_ans_glove:cc1,\n",
    "                                     hyperQA_ELMo.query_elmo:aa3,\n",
    "                                     hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                     hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                     hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                     hyperQA_ELMo.neg_ans_elmo:cc3,\n",
    "                                     hyperQA_ELMo.neg_ans_elmo_tokens:cc5,\n",
    "                                     hyperQA_ELMo.query_bmask:aa4,\n",
    "                                     hyperQA_ELMo.pos_ans_bmask:bb4,\n",
    "                                     hyperQA_ELMo.neg_ans_bmask:cc4})  \n",
    "        cum_loss+=result[0];\n",
    "        cum_pos_loss+=result[1];\n",
    "        cum_neg_loss+=result[2];\n",
    "        frm+=VALIDATION_BATCH_SIZE;\n",
    "        progressBar(frm,n_val,['loss','pos_loss','neg_loss'],[result[0],result[1],result[2]]);\n",
    "      print('\\nVALIDATION || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r879ahcT2CE4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "tf.reset_default_graph();\n",
    "elmo = hub.Module(\"elmo2\", trainable=True)\n",
    "#def ElmoEmbedding(x_tf): #x_tf is a tensor \n",
    "#    return elmo(x_tf, as_dict = True, signature='default')['elmo']\n",
    "def ElmoEmbedding(tokens_input, tokens_length): #inputs here are tensors \n",
    "    return elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature=\"tokens\",as_dict=True)[\"elmo\"]\n",
    "    \n",
    "#embeddings = elmo([\"the cat is on the mat\", \"dogs are in the fog\"],signature=\"default\",as_dict=True)[\"elmo\"]\n",
    "#embeddings\n",
    "\n",
    "test_batch = ['How are you doing?','What is your age?', 'How to kill a cockroach..damn'];\n",
    "glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch) \n",
    "for item in [glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens]:\n",
    "    print(item)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPuDks6o2HB-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "glove_inp = tf.placeholder(tf.int32, shape=(None, None))\n",
    "elmo_inp = tf.placeholder(tf.string, shape=(None, None))\n",
    "elmo_token_len = tf.placeholder(tf.int32, shape=(None))\n",
    "\n",
    "embedding_tf = tf.Variable(dfg.values , trainable=False, dtype=tf.float32, name='embeddings')\n",
    "glove_emb = tf.nn.embedding_lookup(embedding_tf, glove_inp) # [None,None,emb_dim]\n",
    "elmo_emb = ElmoEmbedding(elmo_inp, elmo_token_len);\n",
    "final_emb = tf.concat([glove_emb,elmo_emb],axis=-1,name='concat')\n",
    "\n",
    "sess = tf.Session(config=tf_sess_config);\n",
    "sess.run(tf.global_variables_initializer());\n",
    "sess.run(tf.tables_initializer());\n",
    "check = sess.run([final_emb,glove_emb,elmo_emb],feed_dict={glove_inp:glove_arr,elmo_inp:elmo_arr,elmo_token_len:n_tokens})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSJ8KNBF2KAA"
   },
   "outputs": [],
   "source": [
    "# args\n",
    "EVAL_BATCH_SIZE = 32;\n",
    "\n",
    "# load data\n",
    "df_test = pd.read_csv(\"eval1_unlabelled.tsv\", sep= '\\t', header=None)\n",
    "print('Eval Data Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0acJVWw-2NCy"
   },
   "outputs": [],
   "source": [
    "# get scores\n",
    "outputs = [];\n",
    "with tf.Session(config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  saver.restore(sess, r'./'+model_save_folder+'/epoch_model_1.ckpt');\n",
    "  for i, row in df_test.iterrows():\n",
    "    if i==0:\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    if i!=0 and i%EVAL_BATCH_SIZE==0:\n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(batch_passages,infer_max_len=True)\n",
    "      result = sess.run([hyperQA_ELMo.p_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                          hyperQA_ELMo.query_glove:aa1,\n",
    "                                                          hyperQA_ELMo.pos_ans_glove:bb1,\n",
    "                                                          hyperQA_ELMo.query_elmo:aa3,\n",
    "                                                          hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                                          hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                                          hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                                          hyperQA_ELMo.query_bmask:aa4,\n",
    "                                                          hyperQA_ELMo.pos_ans_bmask:bb4})\n",
    "      outputs.append(result[0].tolist());\n",
    "      progressBar(i,df_test.shape[0]);\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    query, passage = row[1], row[2];\n",
    "    batch_queries.append(query);\n",
    "    batch_passages.append(passage);\n",
    "  if batch_queries:\n",
    "    #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "    aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "    bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(batch_passages,infer_max_len=True)\n",
    "    result = sess.run([hyperQA_ELMo.p_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                        hyperQA_ELMo.query_glove:aa1,\n",
    "                                                        hyperQA_ELMo.pos_ans_glove:bb1,\n",
    "                                                        hyperQA_ELMo.query_elmo:aa3,\n",
    "                                                        hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                                        hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                                        hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                                        hyperQA_ELMo.query_bmask:aa4,\n",
    "                                                        hyperQA_ELMo.pos_ans_bmask:bb4})\n",
    "    outputs.append(result[0].tolist());\n",
    "    progressBar(i,df_test.shape[0]);\n",
    "    batch_queries = [];\n",
    "    batch_passages = [];\n",
    "outputs = np.vstack(outputs)\n",
    "df_test['pred'] = outputs; # the smaller the score, the more positive_answer it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4D8bl3ob2PQ6"
   },
   "outputs": [],
   "source": [
    "# re-using code from Dileep\n",
    "outfilename = 'answer.tsv'\n",
    "with open(outfilename,\"w\",encoding=\"utf-8\") as fw:\n",
    "  import math\n",
    "  linelist = []\n",
    "  tempscores = []\n",
    "  for idx, row in tqdm(df_test.iterrows()):\n",
    "      tempscores.append(row['pred'])\n",
    "      #tempscores.append(str(row['pred']))\n",
    "      if((idx +1)%10==0):\n",
    "          tempscores-=np.min(tempscores);\n",
    "          tempscores = [math.exp(s) for s in tempscores];\n",
    "          expsum = sum(tempscores)\n",
    "          tempscores = [str(s/expsum) for s in tempscores]\n",
    "          scoreString = \"\\t\".join(tempscores)\n",
    "          qid = str(row[0])\n",
    "          fw.write(qid+\"\\t\"+scoreString+\"\\n\")\n",
    "          tempscores=[]\n",
    "      #if(idx%10000==0):\n",
    "      #    print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67CAw6pZ_Rnm"
   },
   "source": [
    "# HyperQA Training from Scrath with ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save \n",
    "!mkdir hyperQA_elmo_500\n",
    "model_save_folder = r'hyperQA_elmo_500'\n",
    "dfile = open('./'+model_save_folder+'/dummy.txt','w',encoding='utf-8');\n",
    "dfile.write('Checking...');\n",
    "dfile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_elmo_batch_data(batch, infer_max_len=True, max_len=0): # batch is list of sentence/paragraph string\n",
    "  elmo_arr, elmo_bmasks, n_tokens = [], [], [];\n",
    "  cstrg_, cstrg_len_ = [], [];\n",
    "  for row in batch:\n",
    "    cstrg__, cstrg_len__ = cleanText(row);\n",
    "    cstrg_.append(cstrg__);\n",
    "    cstrg_len_.append(cstrg_len__);\n",
    "  max_len = np.max(cstrg_len_) if infer_max_len else max_len;\n",
    "  for i, cstrg in enumerate(cstrg_):\n",
    "    cstrg_elmo = [];    \n",
    "    if cstrg_len_[i]>=max_len:\n",
    "      cstrg_elmo = cstrg[:max_len].copy();\n",
    "      n_tokens.append(max_len);\n",
    "    elif cstrg_len_[i]<max_len:\n",
    "      cstrg_elmo = cstrg.copy(); \n",
    "      cstrg_elmo+=[UNKNOWN_ELMO]*(max_len-cstrg_len_[i])\n",
    "      n_tokens.append(cstrg_len_[i]);\n",
    "    # as per elmo requirement\n",
    "    elmo_arr.append(cstrg_elmo);\n",
    "    bmask_elmo = [0.0 if word==UNKNOWN_ELMO else 1.0 for word in cstrg_elmo];\n",
    "    elmo_bmasks.append(bmask_elmo)\n",
    "  return elmo_arr, elmo_bmasks, n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfMsjGFX_U6r"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDtXL_Eh_itd"
   },
   "outputs": [],
   "source": [
    "# HyperQA Tensorflow model\n",
    "\n",
    "class HyperQA_ELMo(object):\n",
    "    def __init__(self, projection_dim, margin, lr=0.001, elmo=None, elmo_emb_dim=1024):\n",
    "\n",
    "        # parameters and variables\n",
    "        self.projection_dim = projection_dim;\n",
    "        self.margin = margin;\n",
    "        self.lr = lr;\n",
    "        self.elmo = elmo;\n",
    "        self.elmo_emb_dim = self.concat_dim = elmo_emb_dim;\n",
    "        self.init_name = tf.contrib.layers.xavier_initializer()\n",
    "        self.w_p = tf.get_variable(\"w_p\", shape=[self.concat_dim,self.projection_dim], initializer=self.init_name, trainable=True, dtype=tf.float32)\n",
    "        self.b_p = tf.get_variable('b_p', shape=[self.projection_dim], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.w_f = tf.get_variable(\"w_f\", shape=[1], initializer=self.init_name, trainable=True, dtype=tf.float32)\n",
    "        self.b_f = tf.get_variable('b_f', shape=[1], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "\n",
    "        # placeholder: general\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.reg_lambda = tf.placeholder(tf.float32, [])\n",
    "        # placeholder: elmo\n",
    "        self.query_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.query_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.pos_ans_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.pos_ans_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.neg_ans_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.neg_ans_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        # placeholder: masking\n",
    "        self.query_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.pos_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.neg_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "\n",
    "        # get elmo embeddings # [None,None,elmo_emb_dim]\n",
    "        self.query_elmo_emb = self.ElmoEmbedding(self.query_elmo, self.query_elmo_tokens);\n",
    "        self.pos_ans_elmo_emb = self.ElmoEmbedding(self.pos_ans_elmo, self.pos_ans_elmo_tokens);\n",
    "        self.neg_ans_elmo_emb = self.ElmoEmbedding(self.neg_ans_elmo, self.neg_ans_elmo_tokens);\n",
    "\n",
    "        # concat embeddings # [None,None,concat_dim]\n",
    "        self.query_emb = self.query_elmo_emb\n",
    "        self.pos_ans_emb = self.pos_ans_elmo_emb\n",
    "        self.neg_ans_emb = self.neg_ans_elmo_emb\n",
    "\n",
    "        # projections with input # [None,None,glove_emb_dim] and output # [None,None,proj_emb]\n",
    "        self.query_proj_ = self.project_fn(self.query_emb, self.query_bmask);\n",
    "        self.pos_ans_proj_ = self.project_fn(self.pos_ans_emb, self.pos_ans_bmask);\n",
    "        self.neg_ans_proj_ = self.project_fn(self.neg_ans_emb, self.neg_ans_bmask);\n",
    "        # dropout\n",
    "        self.query_proj = tf.nn.dropout(self.query_proj_, self.keep_prob);\n",
    "        self.pos_ans_proj = tf.nn.dropout(self.pos_ans_proj_, self.keep_prob);\n",
    "        self.neg_ans_proj = tf.nn.dropout(self.neg_ans_proj_, self.keep_prob);\n",
    "        # unit normalized representations with output #[None,proj_emb]\n",
    "        self.query_vec = tf.clip_by_norm(tf.reduce_sum(self.query_proj,axis=1), 1.0, axes=1) \n",
    "        self.pos_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.pos_ans_proj,axis=1), 1.0, axes=1)\n",
    "        self.neg_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.neg_ans_proj,axis=1), 1.0, axes=1)\n",
    "        # hyperbolic distance\n",
    "        self.p_distance = self.hyperbolic_ball(self.query_vec, self.pos_ans_vec) #[None,1]\n",
    "        self.n_distance = self.hyperbolic_ball(self.query_vec, self.neg_ans_vec) #[None,1]\n",
    "        # loss\n",
    "        self.p_score = self.p_distance*self.w_f+self.b_f; \n",
    "        self.n_score = self.n_distance*self.w_f+self.b_f;\n",
    "        self.losses = tf.nn.relu(self.margin + self.n_score - self.p_score) #[None,1]\n",
    "        self.reg_losses = self.reg_lambda*tf.reduce_sum(tf.abs(self.w_p));\n",
    "        self.loss = self.reg_losses+tf.reduce_sum(self.losses) #[]\n",
    "        # print loss ops\n",
    "        self.print_p_distance = tf.reduce_mean(self.p_distance)\n",
    "        self.print_n_distance = tf.reduce_mean(self.n_distance)\n",
    "        self.print_p_score_loss = tf.reduce_mean(self.p_score)\n",
    "        self.print_n_score_loss = tf.reduce_mean(self.n_score)\n",
    "        self.print_losses = tf.reduce_mean(self.losses)\n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        # adjust gradient\n",
    "        gradients = self.optimizer.compute_gradients(self.loss)\n",
    "        reim_gradients = [(self._to_riemannian_gradient(grad), var) for grad, var in gradients]\n",
    "        clip_gradients = [(self._ClipIfNotNone(grad), var) for grad, var in reim_gradients]\n",
    "        self.train_op = self.optimizer.apply_gradients(clip_gradients)  \n",
    "\n",
    "    def ElmoEmbedding(self, tokens_input, tokens_length): #inputs here are tensors \n",
    "        return self.elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature=\"tokens\",as_dict=True)[\"elmo\"]     \n",
    "    def project_fn(self, inp_emb, bmask): # Input Shape [None,None,concat_dim], [None,None]\n",
    "        runtime_shape = tf.shape(inp_emb);\n",
    "        dim1 = runtime_shape[0];\n",
    "        dim2 = runtime_shape[1];\n",
    "        dense_output = tf.nn.xw_plus_b(tf.reshape(inp_emb, [-1,self.concat_dim]), self.w_p, self.b_p);\n",
    "        activated_output = tf.nn.relu(dense_output);\n",
    "        proj_emb = tf.reshape(activated_output,[dim1,dim2,self.projection_dim]);\n",
    "        bmask = tf.tile(tf.expand_dims(bmask,axis=-1),[1,1,self.projection_dim]);\n",
    "        masked_proj_emb = bmask*proj_emb;\n",
    "        return masked_proj_emb\n",
    "    def hyperbolic_ball(self, x, y, neg=False, eps=1E-6):\n",
    "        \"\"\" Poincare Distance Function \"\"\"\n",
    "        z = x - y\n",
    "        z = tf.norm(z, ord='euclidean', keep_dims=True, axis=1)\n",
    "        z = tf.square(z)\n",
    "        x_d = 1 - tf.square(tf.norm(x, ord='euclidean', keep_dims=True, axis=1))\n",
    "        y_d = 1 - tf.square(tf.norm(y, ord='euclidean', keep_dims=True, axis=1))\n",
    "        d = x_d * y_d\n",
    "        z = z / (d + eps)\n",
    "        z  = (2 * z) + 1\n",
    "        arcosh = z + tf.sqrt(tf.square(z) - 1 + eps)\n",
    "        arcosh = tf.log(arcosh)\n",
    "        if(neg):\n",
    "            arcosh = -arcosh\n",
    "        return arcosh\n",
    "    def _ClipIfNotNone(self, grad):\n",
    "        if grad is None:\n",
    "          return grad\n",
    "        grad = tf.clip_by_value(grad, -10, 10, name=None)\n",
    "        #grad = tf.clip_by_norm(grad, 1.0)\n",
    "        return grad\n",
    "    def _to_riemannian_gradient(self, ge):\n",
    "      if ge is None:\n",
    "        return None\n",
    "      try:\n",
    "        shape = ge.get_shape().as_list()\n",
    "        if len(shape) >= 3:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, axis=[-2, -1], keepdims=True))\n",
    "        elif len(shape) == 2:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, keepdims=True))\n",
    "        else:\n",
    "            return ge\n",
    "      except:\n",
    "        print('Exception handled!')\n",
    "        grad_scale = 1 - tf.square(tf.norm(ge, keep_dims=True))\n",
    "      grad_scale = (tf.square(grad_scale) + 1e-10) / 4.0\n",
    "      gr = ge * grad_scale\n",
    "      # gr = tf.clip_by_norm(gr, 1.0, axes=0)\n",
    "      return gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JGY5rJnLBmWu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "graph built\n"
     ]
    }
   ],
   "source": [
    "# some global vars\n",
    "PROJ_DIMS = 500;\n",
    "MARGIN = 5.0;\n",
    "LEARNING_RATE = 0.001;\n",
    "\n",
    "# Create a graph with given params\n",
    "tf.reset_default_graph();\n",
    "elmo = hub.Module(\"elmo2\", trainable=True)\n",
    "hyperQA_ELMo = HyperQA_ELMo(PROJ_DIMS, MARGIN, LEARNING_RATE, elmo=elmo, elmo_emb_dim=1024)\n",
    "print('graph built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEILTye8DxKo"
   },
   "outputs": [],
   "source": [
    "cleanText = cleanText_Spaces_;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_zpgJKw__o7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning epoch: 0\n",
      "Percent: [------------------->][419328/419350] 100% ||  LOSS: 3.484532 | POS_LOSS: 90.283737 | NEG_LOSS: 87.830597 | \n",
      "TRAINING || mean loss: 3.6658072012484584 | mean pos loss: 63.62864209960814 | mean neg loss: 61.69355416807354\n",
      "beginning epoch: 1\n",
      "Percent: [>                   ][12160/419350] 3% ||  LOSS: 5.079266 | POS_LOSS: 87.743538 | NEG_LOSS: 87.822807 | "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-82ffe3d74563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m                                                                 \u001b[0mhyperQA_ELMo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_ans_elmo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbb3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                                                 \u001b[0mhyperQA_ELMo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_ans_bmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbb4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                                                                 hyperQA_ELMo.neg_ans_elmo_tokens:bb5}) \n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m# array of batch size i.e. (BS,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NEW: max sampling from https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_CIKM2016.pdf\n",
    "# Run this cell for both random_ as well as max_sampling\n",
    "choice_max = 1; # the greater this is, the more cprob to chosse MAX over RANDOM\n",
    "default_sampling_choice = 'RANDOM';\n",
    "RESUME_TRAINING = True;\n",
    "START_EPOCH = 1;\n",
    "TRAINING_BATCH_SIZE = 32;\n",
    "VALIDATION_BATCH_SIZE = 32;\n",
    "VALIDATION_EVERY = 101;\n",
    "N_EPOCHS = 100;\n",
    "\n",
    "# Run the connections for desired epochs\n",
    "with tf.Session() as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  if not RESUME_TRAINING:\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    START_EPOCH = 0;\n",
    "  else:\n",
    "    saver.restore(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(START_EPOCH-1));\n",
    "  for epoch in np.arange(START_EPOCH, N_EPOCHS):\n",
    "    # sampling choice\n",
    "    if epoch==0:\n",
    "      sampling_choice = default_sampling_choice;\n",
    "    else:\n",
    "      sampling_choice = 'MAX' if np.random.random()<choice_max else default_sampling_choice;\n",
    "    # Loss and optimization on Training Data\n",
    "    print('beginning epoch: {}'.format(epoch));\n",
    "    n_batches = int(np.floor(n_train/TRAINING_BATCH_SIZE));\n",
    "    cum_loss = 0;\n",
    "    cum_pos_loss = 0;\n",
    "    cum_neg_loss = 0;\n",
    "    frm=0;\n",
    "    # TRAIN\n",
    "    for i in range(n_batches):\n",
    "      if i%1000==0:\n",
    "        saver.save(sess, './'+model_save_folder+'/batch_model.ckpt'.format(epoch));\n",
    "      batch_query_ids = training_query_ids[frm:frm+TRAINING_BATCH_SIZE];\n",
    "      batch_queries = [];\n",
    "      batch_pos = [];\n",
    "      batch_neg = [];\n",
    "      batch_arg_neg = [];\n",
    "      # TRAIN: Find the most challenging negative!\n",
    "      if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "        unfort_index = 0;\n",
    "        while(unfort_index<len(batch_query_ids)):\n",
    "            small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "            dummy_batch_queries = [];\n",
    "            dummy_batch_neg = [];\n",
    "            dummy_batch_neg_n = [];\n",
    "            for query_id in small_batch_query_ids:\n",
    "              dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "              for jj in data_dict[query_id]['negs']:\n",
    "                dummy_batch_neg.append(id2passage[jj]);\n",
    "              dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))        \n",
    "            # elmo_arr, elmo_bmasks, n_tokens = make_elmo_batch_data(test_batch)\n",
    "            aa3, aa4, aa5 = make_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "            bb3, bb4, bb5 = make_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "            aa3_new, aa4_new, aa5_new = [], [], [];\n",
    "            for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times\n",
    "              aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "              aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "              aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "            aa3_new, aa4_new, aa5_new = np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "            aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "            result = sess.run([hyperQA_ELMo.n_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                                hyperQA_ELMo.query_elmo:aa3_new,\n",
    "                                                                hyperQA_ELMo.query_bmask:aa4_new,\n",
    "                                                                hyperQA_ELMo.query_elmo_tokens:aa5_new,\n",
    "                                                                hyperQA_ELMo.neg_ans_elmo:bb3,\n",
    "                                                                hyperQA_ELMo.neg_ans_bmask:bb4,\n",
    "                                                                hyperQA_ELMo.neg_ans_elmo_tokens:bb5}) \n",
    "            result = result[0]; # array of batch size i.e. (BS,1)\n",
    "            k=0;\n",
    "            for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "              batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "              k+=n_times;\n",
    "            unfort_index+=3;\n",
    "      # TRAIN: compute loss and optimize\n",
    "      if sampling_choice=='MAX':\n",
    "        assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "      for i_id, query_id in enumerate(batch_query_ids):\n",
    "        batch_queries.append(data_dict[query_id]['query']);\n",
    "        batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "        this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "        batch_neg.append(this_negative);\n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      aa3, aa4, aa5 = make_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb3, bb4, bb5 = make_elmo_batch_data(batch_pos,infer_max_len=True) \n",
    "      cc3, cc4, cc5 = make_elmo_batch_data(batch_neg,infer_max_len=True)\n",
    "      result = sess.run([hyperQA_ELMo.train_op, hyperQA_ELMo.print_losses, hyperQA_ELMo.print_p_score_loss, hyperQA_ELMo.print_n_score_loss],\n",
    "                        feed_dict={hyperQA_ELMo.keep_prob: 0.8,\n",
    "                                   hyperQA_ELMo.reg_lambda: 0.00001,\n",
    "                                   hyperQA_ELMo.query_elmo:aa3,\n",
    "                                   hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                   hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                   hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                   hyperQA_ELMo.neg_ans_elmo:cc3,\n",
    "                                   hyperQA_ELMo.neg_ans_elmo_tokens:cc5,\n",
    "                                   hyperQA_ELMo.query_bmask:aa4,\n",
    "                                   hyperQA_ELMo.pos_ans_bmask:bb4,\n",
    "                                   hyperQA_ELMo.neg_ans_bmask:cc4})    \n",
    "      cum_loss+=result[1];\n",
    "      cum_pos_loss+=result[2];\n",
    "      cum_neg_loss+=result[3];\n",
    "      frm+=TRAINING_BATCH_SIZE;\n",
    "      progressBar(frm,n_train,['loss','pos_loss','neg_loss'],[result[1],result[2],result[3]]);\n",
    "    print('\\nTRAINING || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))\n",
    "    saver.save(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(epoch));\n",
    "    # VALIDATE\n",
    "    if epoch!=0 and epoch%VALIDATION_EVERY==0:\n",
    "      #saver = tf.train.Saver();\n",
    "      #saver.restore(val_sess, './'+model_save_folder+'/epoch_model_20.ckpt');\n",
    "      n_batches = int(np.floor(n_val/VALIDATION_BATCH_SIZE));\n",
    "      cum_loss = 0;\n",
    "      cum_pos_loss = 0;\n",
    "      cum_neg_loss = 0;\n",
    "      frm=0;\n",
    "      for i in range(n_batches):\n",
    "        batch_query_ids = validation_query_ids[frm:frm+VALIDATION_BATCH_SIZE];\n",
    "        batch_queries = [];\n",
    "        batch_pos = [];\n",
    "        batch_neg = [];\n",
    "        batch_arg_neg = [];\n",
    "        # VALIDATE: Find the most challenging negative!\n",
    "        if sampling_choice=='MAX' and False: #get j's which are maximums in their negative sets\n",
    "            unfort_index = 0;\n",
    "            while(unfort_index<len(batch_query_ids)):\n",
    "                small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "                dummy_batch_queries = [];\n",
    "                dummy_batch_neg = [];\n",
    "                dummy_batch_neg_n = [];\n",
    "                for query_id in small_batch_query_ids:\n",
    "                  dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "                  for jj in data_dict[query_id]['negs']:\n",
    "                    dummy_batch_neg.append(id2passage[jj]);\n",
    "                  dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))                        \n",
    "                # elmo_arr, elmo_bmasks, n_tokens = make_elmo_batch_data(test_batch)\n",
    "                aa3, aa4, aa5 = make_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "                bb3, bb4, bb5 = make_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "                aa3_new, aa4_new, aa5_new = [], [], [];\n",
    "                for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times\n",
    "                  aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "                  aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "                  aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "                aa3_new, aa4_new, aa5_new = np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "                aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "                result = sess.run([hyperQA_ELMo.n_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                                    hyperQA_ELMo.query_elmo:aa3_new,\n",
    "                                                                    hyperQA_ELMo.query_bmask:aa4_new,\n",
    "                                                                    hyperQA_ELMo.query_elmo_tokens:aa5_new,\n",
    "                                                                    hyperQA_ELMo.neg_ans_elmo:bb3,\n",
    "                                                                    hyperQA_ELMo.neg_ans_bmask:bb4,\n",
    "                                                                    hyperQA_ELMo.neg_ans_elmo_tokens:bb5})\n",
    "                result = result[0]; # array of batch size i.e. (BS,1)\n",
    "                k=0;\n",
    "                for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "                  batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "                  k+=n_times;\n",
    "                unfort_index+=3;\n",
    "        # VALIDATE: compute loss only\n",
    "        if sampling_choice=='MAX':\n",
    "          assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "        for i_id, query_id in enumerate(batch_query_ids):\n",
    "          batch_queries.append(data_dict[query_id]['query']);\n",
    "          batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "          this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "          batch_neg.append(this_negative);\n",
    "        #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "        aa3, aa4, aa5 = make_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "        bb3, bb4, bb5 = make_elmo_batch_data(batch_pos,infer_max_len=True) \n",
    "        cc3, cc4, cc5 = make_elmo_batch_data(batch_neg,infer_max_len=True)\n",
    "        result = sess.run([hyperQA_ELMo.print_losses, hyperQA_ELMo.print_p_score_loss, hyperQA_ELMo.print_n_score_loss],\n",
    "                        feed_dict={hyperQA_ELMo.keep_prob: 1,\n",
    "                                   hyperQA_ELMo.query_elmo:aa3,\n",
    "                                   hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                   hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                   hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                   hyperQA_ELMo.neg_ans_elmo:cc3,\n",
    "                                   hyperQA_ELMo.neg_ans_elmo_tokens:cc5,\n",
    "                                   hyperQA_ELMo.query_bmask:aa4,\n",
    "                                   hyperQA_ELMo.pos_ans_bmask:bb4,\n",
    "                                   hyperQA_ELMo.neg_ans_bmask:cc4})   \n",
    "        cum_loss+=result[0];\n",
    "        cum_pos_loss+=result[1];\n",
    "        cum_neg_loss+=result[2];\n",
    "        frm+=VALIDATION_BATCH_SIZE;\n",
    "        progressBar(frm,n_val,['loss','pos_loss','neg_loss'],[result[0],result[1],result[2]]);\n",
    "      print('\\nVALIDATION || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2sFyjGb6EqU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Data Loaded.\n"
     ]
    }
   ],
   "source": [
    "# args\n",
    "EVAL_BATCH_SIZE = 32;\n",
    "\n",
    "# load data\n",
    "df_test = pd.read_csv(\"eval1_unlabelled.tsv\", sep= '\\t', header=None)\n",
    "print('Eval Data Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBtqGHYl6Jy0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./hyperQA_elmo_500/epoch_model_0.ckpt\n",
      "Percent: [------------------->][104169/104170] 100% ||  : -1.000000 | "
     ]
    }
   ],
   "source": [
    "# get scores\n",
    "outputs = [];\n",
    "with tf.Session(config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  saver.restore(sess, r'./'+model_save_folder+'/epoch_model_0.ckpt');\n",
    "  for i, row in df_test.iterrows():\n",
    "    if i==0:\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    if i!=0 and i%EVAL_BATCH_SIZE==0:\n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      aa3, aa4, aa5 = make_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb3, bb4, bb5 = make_elmo_batch_data(batch_passages,infer_max_len=True)\n",
    "      result = sess.run([hyperQA_ELMo.p_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                          hyperQA_ELMo.query_elmo:aa3,\n",
    "                                                          hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                                          hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                                          hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                                          hyperQA_ELMo.query_bmask:aa4,\n",
    "                                                          hyperQA_ELMo.pos_ans_bmask:bb4})\n",
    "      outputs.append(result[0].tolist());\n",
    "      progressBar(i,df_test.shape[0]);\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    query, passage = row[1], row[2];\n",
    "    batch_queries.append(query);\n",
    "    batch_passages.append(passage);\n",
    "  if batch_queries:\n",
    "    #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "    aa3, aa4, aa5 = make_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "    bb3, bb4, bb5 = make_elmo_batch_data(batch_passages,infer_max_len=True)\n",
    "    result = sess.run([hyperQA_ELMo.p_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                          hyperQA_ELMo.query_elmo:aa3,\n",
    "                                                          hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                                          hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                                          hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                                          hyperQA_ELMo.query_bmask:aa4,\n",
    "                                                          hyperQA_ELMo.pos_ans_bmask:bb4})    \n",
    "    outputs.append(result[0].tolist());\n",
    "    progressBar(i,df_test.shape[0]);\n",
    "    batch_queries = [];\n",
    "    batch_passages = [];\n",
    "outputs = np.vstack(outputs)\n",
    "df_test['pred'] = outputs; # the smaller the score, the more positive_answer it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnZLz7Vp6Q-d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104170it [00:08, 11948.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# re-using code from Dileep\n",
    "outfilename = 'hyperQA_elmo_500_epoch_0.tsv'\n",
    "with open(outfilename,\"w\",encoding=\"utf-8\") as fw:\n",
    "  import math\n",
    "  linelist = []\n",
    "  tempscores = []\n",
    "  for idx, row in tqdm(df_test.iterrows()):\n",
    "      tempscores.append(row['pred'])\n",
    "      #tempscores.append(str(row['pred']))\n",
    "      if((idx +1)%10==0):\n",
    "          tempscores-=np.min(tempscores);\n",
    "          tempscores = [math.exp(s) for s in tempscores];\n",
    "          expsum = sum(tempscores)\n",
    "          tempscores = [str(s/expsum) for s in tempscores]\n",
    "          scoreString = \"\\t\".join(tempscores)\n",
    "          qid = str(row[0])\n",
    "          fw.write(qid+\"\\t\"+scoreString+\"\\n\")\n",
    "          tempscores=[]\n",
    "      #if(idx%10000==0):\n",
    "      #    print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OcqYS24w6Xyk"
   },
   "source": [
    "# BiLSTM-Attention-SiameseLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save \n",
    "!mkdir biLSTM_Attention\n",
    "model_save_folder = r'biLSTM_Attention'\n",
    "dfile = open('./'+model_save_folder+'/dummy.txt','w',encoding='utf-8');\n",
    "dfile.write('Checking...');\n",
    "dfile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Funcs\n",
    "def make_glove_batch_data(batch, infer_max_len=True, max_len=0): # batch is list of sentence/paragraph string\n",
    "  glove_arr, glove_bmasks, n_tokens = [], [], [];\n",
    "  cstrg_, cstrg_len_ = [], [];\n",
    "  for row in batch:\n",
    "    cstrg__, cstrg_len__ = cleanText(row);\n",
    "    cstrg_.append(cstrg__);\n",
    "    cstrg_len_.append(cstrg_len__);\n",
    "  max_len = np.max(cstrg_len_) if infer_max_len else max_len;\n",
    "  for i, cstrg in enumerate(cstrg_):\n",
    "    cstrg_glove = [];    \n",
    "    if cstrg_len_[i]>=max_len:\n",
    "      cstrg_glove = cstrg[:max_len].copy();\n",
    "      n_tokens.append(max_len);\n",
    "    elif cstrg_len_[i]<max_len:\n",
    "      cstrg_glove = cstrg.copy(); \n",
    "      cstrg_glove+=[UNKNOWN_GLOVE]*(max_len-cstrg_len_[i])\n",
    "      n_tokens.append(cstrg_len_[i]);\n",
    "    # as per glove requirement\n",
    "    cstrg_glove = [word if word in word2id.keys() else UNKNOWN_GLOVE for word in cstrg_glove];\n",
    "    bmask_glove = [0.0 if word==UNKNOWN_GLOVE else 1.0 for word in cstrg_glove];\n",
    "    cstrg_glove = [word2id[word] for word in cstrg_glove];\n",
    "    glove_arr.append(cstrg_glove);\n",
    "    glove_bmasks.append(bmask_glove);\n",
    "  return np.vstack(glove_arr), np.vstack(glove_bmasks), n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fv8vNj5m6gpy"
   },
   "outputs": [],
   "source": [
    "# HyperQA Tensorflow model\n",
    "\n",
    "class BiLSTM_Attention(object):\n",
    "    def __init__(self, vocab_embedding, glove_emb_dim, margin, lr=0.001):\n",
    "\n",
    "        # parameters and variables\n",
    "        self.glove_embedding = tf.Variable(vocab_embedding, trainable=False, dtype=tf.float32, name='glove_embedding');\n",
    "        self.glove_emb_dim = glove_emb_dim;\n",
    "        self.margin = margin;\n",
    "        self.lr = lr;\n",
    "\n",
    "        # placeholder: general\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.reg_lambda = tf.placeholder(tf.float32, [])\n",
    "        # placeholder: glove\n",
    "        self.query_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        self.pos_ans_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        self.neg_ans_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        # placeholder: tokens\n",
    "        self.query_glove_seq_lengths = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.pos_ans_glove_seq_lengths = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.neg_ans_glove_seq_lengths = tf.placeholder(tf.int32, shape=(None));\n",
    "        # placeholder: masking\n",
    "        #self.query_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        #self.pos_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        #self.neg_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        \n",
    "        # layer: glove embedding # [None,None,glove_emb_dim]\n",
    "        self.query_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.query_glove)\n",
    "        self.pos_ans_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.pos_ans_glove)\n",
    "        self.neg_ans_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.neg_ans_glove)\n",
    "        \n",
    "        # layer: BiLSTM with 2 layers # time_major=False # [None,None,cell_fw.ouput_size+cell_bw.output_size]\n",
    "        with tf.variable_scope(\"query_BiLSTM\"):\n",
    "          self.query_BiLSTM = self.query_BiLSTM_fn(self.query_glove_emb,self.query_glove_seq_lengths)\n",
    "        with tf.variable_scope(\"answer_BiLSTM\", reuse = tf.AUTO_REUSE):\n",
    "          self.pos_ans_BiLSTM = self.answer_BiLSTM_fn(self.pos_ans_glove_emb,self.pos_ans_glove_seq_lengths)\n",
    "          self.neg_ans_BiLSTM = self.answer_BiLSTM_fn(self.neg_ans_glove_emb,self.neg_ans_glove_seq_lengths)\n",
    " \n",
    "        # layer: Attention+SiameseEncoder\n",
    "        self.p_score = self.get_score_fn(self.query_BiLSTM, self.query_glove_seq_lengths, self.pos_ans_BiLSTM, self.pos_ans_glove_seq_lengths);\n",
    "        self.n_score = self.get_score_fn(self.query_BiLSTM, self.query_glove_seq_lengths, self.neg_ans_BiLSTM, self.neg_ans_glove_seq_lengths);\n",
    "      \n",
    "        # layer: Loss and Optimization\n",
    "        self.losses = tf.nn.relu(self.margin + self.n_score - self.p_score) #[None,1]\n",
    "        #self.reg_losses = self.reg_lambda*tf.reduce_sum(tf.abs(self.w_p));\n",
    "        self.loss = tf.reduce_sum(self.losses) # +self.reg_losses\n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        # adjust gradient\n",
    "        gradients = self.optimizer.compute_gradients(self.loss)\n",
    "        clip_gradients = [(self._ClipIfNotNone(grad), var) for grad, var in gradients]\n",
    "        self.train_op = self.optimizer.apply_gradients(clip_gradients)\n",
    "        \n",
    "        # print loss ops\n",
    "        self.print_p_score_loss = tf.reduce_mean(self.p_score)\n",
    "        self.print_n_score_loss = tf.reduce_mean(self.n_score)\n",
    "        self.print_losses = tf.reduce_mean(self.losses)\n",
    "    \n",
    "    # Function Definitions\n",
    "    def query_BiLSTM_fn(self, query_input, query_seq_lengths): # [None,None,glove_emb_dim]\n",
    "      query_outputs_L1, query_output_states_L1 = tf.nn.bidirectional_dynamic_rnn(tf.nn.rnn_cell.LSTMCell(num_units=128),tf.nn.rnn_cell.LSTMCell(num_units=128),inputs=query_input,sequence_length=query_seq_lengths,dtype=tf.float32,scope='L1')\n",
    "      query_outputs_concat_L1 = tf.concat(query_outputs_L1, 2)\n",
    "      query_outputs_L2, query_output_states_L2 = tf.nn.bidirectional_dynamic_rnn(tf.nn.rnn_cell.LSTMCell(num_units=128),tf.nn.rnn_cell.LSTMCell(num_units=128),inputs=query_outputs_concat_L1,sequence_length=query_seq_lengths,dtype=tf.float32,scope='L2')\n",
    "      query_outputs_concat_L2 = tf.concat(query_outputs_L2, 2)\n",
    "      return query_outputs_concat_L2\n",
    "    def answer_BiLSTM_fn(self, answer_input, answer_seq_lengths): # [None,None,glove_emb_dim]\n",
    "      answer_outputs_L1, answer_output_states_L1 = tf.nn.bidirectional_dynamic_rnn(tf.nn.rnn_cell.LSTMCell(num_units=128),tf.nn.rnn_cell.LSTMCell(num_units=128),inputs=answer_input,sequence_length=answer_seq_lengths,dtype=tf.float32,scope='L1')\n",
    "      answer_outputs_concat_L1 = tf.concat(answer_outputs_L1, 2)\n",
    "      answer_outputs_L2, answer_output_states_L2 = tf.nn.bidirectional_dynamic_rnn(tf.nn.rnn_cell.LSTMCell(num_units=128),tf.nn.rnn_cell.LSTMCell(num_units=128),inputs=answer_outputs_concat_L1,sequence_length=answer_seq_lengths,dtype=tf.float32,scope='L2')\n",
    "      answer_outputs_concat_L2 = tf.concat(answer_outputs_L2, 2)\n",
    "      return answer_outputs_concat_L2\n",
    "    def attention_fn(self, query, answer): # shape of dim1 in both inputs must be equal. # dim2 are time_axis and can be variable length\n",
    "      query_time_steps, answer_time_steps = tf.shape(query)[1], tf.shape(answer)[1];\n",
    "      # attention scores\n",
    "      interaction_matrix = tf.matmul(query, tf.transpose(answer, perm=[0, 2, 1])); # shape [inputs_1.dims1, inputs_1.dims2, inputs_2.dims2]\n",
    "      query_softmax_, answer_softmax_ = tf.nn.softmax(interaction_matrix,axis=2), tf.nn.softmax(interaction_matrix,axis=1)\n",
    "      query_softmax, answer_softmax = tf.expand_dims(query_softmax_, axis=-1), tf.expand_dims(answer_softmax_, axis=-1)\n",
    "      # query_attention\n",
    "      query_expanded = tf.tile(tf.expand_dims(query,axis=-2), [1,1,answer_time_steps,1]);\n",
    "      query_attn_ = query_expanded*query_softmax;\n",
    "      query_attn = tf.reduce_sum(query_attn_, axis=-2);\n",
    "      # answer_attention\n",
    "      answer_expanded = tf.tile(tf.expand_dims(answer,axis=-3), [1,query_time_steps,1,1]);\n",
    "      answer_attn_ = answer_expanded*answer_softmax;\n",
    "      answer_attn = tf.reduce_sum(answer_attn_, axis=-3);\n",
    "      # outputs\n",
    "      output_query = tf.concat([query,query_attn], axis=-1);\n",
    "      output_answer = tf.concat([answer,answer_attn], axis=-1);      \n",
    "      return output_query, output_answer\n",
    "    def siamese_BiLSTM_fn(self, input_seq, input_seq_lengths): # [None,None,2*(cell_fw.ouput_size+cell_bw.output_size)]\n",
    "      outputs_L1, _ = tf.nn.bidirectional_dynamic_rnn(tf.nn.rnn_cell.LSTMCell(num_units=64),tf.nn.rnn_cell.LSTMCell(num_units=64),inputs=input_seq,sequence_length=input_seq_lengths,dtype=tf.float32,scope='L1')\n",
    "      outputs_concat_L1 = tf.concat(outputs_L1, 2)\n",
    "      return outputs_concat_L1    \n",
    "    def extract_axis_1(self, data, ind):\n",
    "      # https://stackoverflow.com/questions/41273361/get-the-last-output-of-a-dynamic-rnn-in-tensorflow\n",
    "      batch_range = tf.range(tf.shape(data)[0])\n",
    "      indices = tf.stack([batch_range, ind], axis=1)\n",
    "      res = tf.gather_nd(data, indices)\n",
    "      return res # [batch_size, num_cells]\n",
    "    def get_score_fn(self, query, query_seq_lengths, answer, answer_seq_lengths):\n",
    "      query_with_attn, answer_with_attn = self.attention_fn(query, answer);\n",
    "      with tf.variable_scope(\"siamese_BiLSTM_encoder\", reuse = tf.AUTO_REUSE):\n",
    "        a1 = self.siamese_BiLSTM_fn(query_with_attn, query_seq_lengths);\n",
    "        a2 = self.extract_axis_1(a1, query_seq_lengths-1)\n",
    "        b1 = self.siamese_BiLSTM_fn(answer_with_attn, answer_seq_lengths);\n",
    "        b2 = self.extract_axis_1(b1, answer_seq_lengths-1)\n",
    "        c = tf.reduce_sum(tf.multiply(a2,b2), axis=-1);\n",
    "      return c\n",
    "    def _ClipIfNotNone(self, grad):\n",
    "        if grad is None:\n",
    "          return grad\n",
    "        #grad = tf.clip_by_value(grad, -10, 10, name=None)\n",
    "        grad = tf.clip_by_norm(grad, 5.0)\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QgV7IcDX6nyv",
    "outputId": "96658bcf-a374-4870-e35e-3eefcba72810"
   },
   "outputs": [],
   "source": [
    "# some global vars\n",
    "MARGIN = 5.0;\n",
    "LEARNING_RATE = 0.001;\n",
    "\n",
    "# Create a graph with given params\n",
    "tf.reset_default_graph();\n",
    "biLSTM_Attention = BiLSTM_Attention(dfg.values, EMB_DIMS, MARGIN, LEARNING_RATE)\n",
    "print('graph built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZfjH0V8A7xrT"
   },
   "outputs": [],
   "source": [
    "cleanText = cleanText_Simple;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "colab_type": "code",
    "id": "1KfPm76C6uQB",
    "outputId": "bb4edea2-f784-46a3-e699-27b1913c90a5"
   },
   "outputs": [],
   "source": [
    "# NEW: max sampling from https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_CIKM2016.pdf\n",
    "# Run this cell for both random_ as well as max_sampling\n",
    "choice_max = 0; # the greater this is, the more cprob to chosse MAX over RANDOM\n",
    "default_sampling_choice = 'RANDOM';\n",
    "RESUME_TRAINING = False;\n",
    "START_EPOCH = 0;\n",
    "TRAINING_BATCH_SIZE = 32;\n",
    "VALIDATION_BATCH_SIZE = 64;\n",
    "VALIDATION_EVERY = 1;\n",
    "N_EPOCHS = 100;\n",
    "\n",
    "# Run the connections for desired epochs\n",
    "with tf.Session(config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  if not RESUME_TRAINING:\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    START_EPOCH = 0;\n",
    "  else:\n",
    "    saver.restore(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(START_EPOCH-1));\n",
    "  for epoch in np.arange(START_EPOCH, N_EPOCHS):\n",
    "    # sampling choice\n",
    "    if epoch==0:\n",
    "      sampling_choice = default_sampling_choice;\n",
    "    else:\n",
    "      sampling_choice = 'MAX' if np.random.random()<choice_max else default_sampling_choice;\n",
    "    # Loss and optimization on Training Data\n",
    "    print('beginning epoch: {}'.format(epoch));\n",
    "    n_batches = int(np.floor(n_train/TRAINING_BATCH_SIZE));\n",
    "    cum_loss = 0;\n",
    "    cum_pos_loss = 0;\n",
    "    cum_neg_loss = 0;\n",
    "    frm=0;\n",
    "    # TRAIN\n",
    "    for i in range(n_batches):\n",
    "      if i%1000==0:\n",
    "        saver.save(sess, './'+model_save_folder+'/batch_model.ckpt');\n",
    "      batch_query_ids = training_query_ids[frm:frm+TRAINING_BATCH_SIZE];\n",
    "      batch_queries = [];\n",
    "      batch_pos = [];\n",
    "      batch_neg = [];\n",
    "      batch_arg_neg = [];\n",
    "      # TRAIN: Find the most challenging negative!\n",
    "      if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "        unfort_index = 0;\n",
    "        while(unfort_index<len(batch_query_ids)):\n",
    "            small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "            dummy_batch_queries = [];\n",
    "            dummy_batch_neg = [];\n",
    "            dummy_batch_neg_n = [];\n",
    "            for query_id in small_batch_query_ids:\n",
    "              dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "              for jj in data_dict[query_id]['negs']:\n",
    "                dummy_batch_neg.append(id2passage[jj]);\n",
    "              dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))        \n",
    "            #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "            aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "            bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "            aa1_new, aa3_new, aa4_new, aa5_new = [], [], [], [];\n",
    "            for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "              aa1_new.append(np.tile(aa1[ii],(n_times,1)));\n",
    "              aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "              aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "              aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "            aa1_new, aa3_new, aa4_new, aa5_new = np.vstack(aa1_new), np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "            aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "            result = sess.run([biLSTM_Attention.n_score],feed_dict={biLSTM_Attention.keep_prob: 1.0,\n",
    "                                                                biLSTM_Attention.query_glove:aa1_new,\n",
    "                                                                biLSTM_Attention.query_elmo:aa3_new,\n",
    "                                                                biLSTM_Attention.query_bmask:aa4_new,\n",
    "                                                                biLSTM_Attention.query_elmo_tokens:aa5_new,\n",
    "                                                                biLSTM_Attention.neg_ans_glove:bb1,\n",
    "                                                                biLSTM_Attention.neg_ans_elmo:bb3,\n",
    "                                                                biLSTM_Attention.neg_ans_bmask:bb4,\n",
    "                                                                biLSTM_Attention.neg_ans_elmo_tokens:bb5})\n",
    "            result = result[0]; # array of batch size i.e. (BS,1)\n",
    "            k=0;\n",
    "            for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "              batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "              k+=n_times;\n",
    "            unfort_index+=3;\n",
    "      # TRAIN: compute loss and optimize\n",
    "      if sampling_choice=='MAX':\n",
    "        assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "      for i_id, query_id in enumerate(batch_query_ids):\n",
    "        batch_queries.append(data_dict[query_id]['query']);\n",
    "        batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "        this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "        batch_neg.append(this_negative);\n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      #glove_arr, glove_bmasks, n_tokens = make_glove_batch_data(test_batch) \n",
    "      aa1, _, aa3 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb1, _, bb3 = make_glove_batch_data(batch_pos,infer_max_len=True) \n",
    "      cc1, _, cc3 = make_glove_batch_data(batch_neg,infer_max_len=True)\n",
    "      result = sess.run([biLSTM_Attention.train_op, biLSTM_Attention.print_losses, biLSTM_Attention.print_p_score_loss, biLSTM_Attention.print_n_score_loss],\n",
    "                        feed_dict={biLSTM_Attention.keep_prob: 0.8,\n",
    "                                   biLSTM_Attention.reg_lambda: 0.00001,\n",
    "                                   biLSTM_Attention.query_glove:aa1,\n",
    "                                   biLSTM_Attention.pos_ans_glove:bb1,\n",
    "                                   biLSTM_Attention.neg_ans_glove:cc1,\n",
    "                                   biLSTM_Attention.query_glove_seq_lengths:aa3,\n",
    "                                   biLSTM_Attention.pos_ans_glove_seq_lengths:bb3,\n",
    "                                   biLSTM_Attention.neg_ans_glove_seq_lengths:cc3})    \n",
    "      cum_loss+=result[1];\n",
    "      cum_pos_loss+=result[2];\n",
    "      cum_neg_loss+=result[3];\n",
    "      frm+=TRAINING_BATCH_SIZE;\n",
    "      progressBar(frm,n_train,['loss','pos_loss','neg_loss'],[result[1],result[2],result[3]]);\n",
    "    print('\\nTRAINING || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))\n",
    "    saver.save(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(epoch));\n",
    "    # VALIDATE\n",
    "    if epoch!=0 and epoch%VALIDATION_EVERY==0:\n",
    "      #saver = tf.train.Saver();\n",
    "      #saver.restore(val_sess, './'+model_save_folder+'/epoch_model_20.ckpt');\n",
    "      n_batches = int(np.floor(n_val/VALIDATION_BATCH_SIZE));\n",
    "      cum_loss = 0;\n",
    "      cum_pos_loss = 0;\n",
    "      cum_neg_loss = 0;\n",
    "      frm=0;\n",
    "      for i in range(n_batches):\n",
    "        batch_query_ids = validation_query_ids[frm:frm+VALIDATION_BATCH_SIZE];\n",
    "        batch_queries = [];\n",
    "        batch_pos = [];\n",
    "        batch_neg = [];\n",
    "        batch_arg_neg = [];\n",
    "        # VALIDATE: Find the most challenging negative!\n",
    "        if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "            unfort_index = 0;\n",
    "            while(unfort_index<len(batch_query_ids)):\n",
    "                small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "                dummy_batch_queries = [];\n",
    "                dummy_batch_neg = [];\n",
    "                dummy_batch_neg_n = [];\n",
    "                for query_id in small_batch_query_ids:\n",
    "                  dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "                  for jj in data_dict[query_id]['negs']:\n",
    "                    dummy_batch_neg.append(id2passage[jj]);\n",
    "                  dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))        \n",
    "                #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "                aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "                bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "                aa1_new, aa3_new, aa4_new, aa5_new = [], [], [], [];\n",
    "                for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "                  aa1_new.append(np.tile(aa1[ii],(n_times,1)));\n",
    "                  aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "                  aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "                  aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "                aa1_new, aa3_new, aa4_new, aa5_new = np.vstack(aa1_new), np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "                aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "                result = sess.run([biLSTM_Attention.n_score],feed_dict={biLSTM_Attention.keep_prob: 1.0,\n",
    "                                                                    biLSTM_Attention.query_glove:aa1_new,\n",
    "                                                                    biLSTM_Attention.query_elmo:aa3_new,\n",
    "                                                                    biLSTM_Attention.query_bmask:aa4_new,\n",
    "                                                                    biLSTM_Attention.query_elmo_tokens:aa5_new,\n",
    "                                                                    biLSTM_Attention.neg_ans_glove:bb1,\n",
    "                                                                    biLSTM_Attention.neg_ans_elmo:bb3,\n",
    "                                                                    biLSTM_Attention.neg_ans_bmask:bb4,\n",
    "                                                                    biLSTM_Attention.neg_ans_elmo_tokens:bb5})\n",
    "                result = result[0]; # array of batch size i.e. (BS,1)\n",
    "                k=0;\n",
    "                for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "                  batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "                  k+=n_times;\n",
    "                unfort_index+=3;\n",
    "        # VALIDATE: compute loss only\n",
    "        if sampling_choice=='MAX':\n",
    "          assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "        for i_id, query_id in enumerate(batch_query_ids):\n",
    "          batch_queries.append(data_dict[query_id]['query']);\n",
    "          batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "          this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "          batch_neg.append(this_negative);\n",
    "        #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "        #glove_arr, glove_bmasks, n_tokens = make_glove_batch_data(test_batch) \n",
    "        aa1, _, aa3 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "        bb1, _, bb3 = make_glove_batch_data(batch_pos,infer_max_len=True) \n",
    "        cc1, _, cc3 = make_glove_batch_data(batch_neg,infer_max_len=True)\n",
    "        result = sess.run([biLSTM_Attention.print_losses, biLSTM_Attention.print_p_score_loss, biLSTM_Attention.print_n_score_loss],\n",
    "                          feed_dict={biLSTM_Attention.keep_prob: 0.8,\n",
    "                                     biLSTM_Attention.reg_lambda: 0.00001,\n",
    "                                     biLSTM_Attention.query_glove:aa1,\n",
    "                                     biLSTM_Attention.pos_ans_glove:bb1,\n",
    "                                     biLSTM_Attention.neg_ans_glove:cc1,\n",
    "                                     biLSTM_Attention.query_glove_seq_lengths:aa3,\n",
    "                                     biLSTM_Attention.pos_ans_glove_seq_lengths:bb3,\n",
    "                                     biLSTM_Attention.neg_ans_glove_seq_lengths:cc3}) \n",
    "        cum_loss+=result[0];\n",
    "        cum_pos_loss+=result[1];\n",
    "        cum_neg_loss+=result[2];\n",
    "        frm+=VALIDATION_BATCH_SIZE;\n",
    "        progressBar(frm,n_val,['loss','pos_loss','neg_loss'],[result[0],result[1],result[2]]);\n",
    "      print('\\nVALIDATION || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvBjcYVxr9QS"
   },
   "outputs": [],
   "source": [
    "# args\n",
    "EVAL_BATCH_SIZE = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hWAGkaMR66_1",
    "outputId": "7ddcb9bd-d06a-4e15-d149-dd416bac82da"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df_test = pd.read_csv(\"eval1_unlabelled.tsv\", sep= '\\t', header=None)\n",
    "print('Eval Data Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4m3eUkR57Ccy",
    "outputId": "9a01a17f-30d4-407e-991e-2cd2696c9fb4"
   },
   "outputs": [],
   "source": [
    "# get scores\n",
    "outputs = [];\n",
    "with tf.Session(config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  #saver.restore(sess, r'./'+model_save_folder+'/epoch_model_0.ckpt');\n",
    "  saver.restore(sess, r'./'+model_save_folder+'/batch_model.ckpt');\n",
    "  for i, row in df_test.iterrows():\n",
    "    if i==0:\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    if i!=0 and i%EVAL_BATCH_SIZE==0:    \n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      #glove_arr, glove_bmasks, n_tokens = make_glove_batch_data(test_batch) \n",
    "      aa1, _, aa3 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb1, _, bb3 = make_glove_batch_data(batch_passages,infer_max_len=True)\n",
    "      result = sess.run([biLSTM_Attention.p_score],\n",
    "                        feed_dict={biLSTM_Attention.keep_prob: 1.0,\n",
    "                                   biLSTM_Attention.query_glove:aa1,\n",
    "                                   biLSTM_Attention.pos_ans_glove:bb1,\n",
    "                                   biLSTM_Attention.query_glove_seq_lengths:aa3,\n",
    "                                   biLSTM_Attention.pos_ans_glove_seq_lengths:bb3})\n",
    "      outputs.append(result[0].tolist());\n",
    "      progressBar(i,df_test.shape[0]);\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    query, passage = row[1], row[2];\n",
    "    batch_queries.append(query);\n",
    "    batch_passages.append(passage);\n",
    "  if batch_queries:\n",
    "    #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "    #glove_arr, glove_bmasks, n_tokens = make_glove_batch_data(test_batch) \n",
    "    aa1, _, aa3 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "    bb1, _, bb3 = make_glove_batch_data(batch_passages,infer_max_len=True)\n",
    "    result = sess.run([biLSTM_Attention.p_score],\n",
    "                      feed_dict={biLSTM_Attention.keep_prob: 1.0,\n",
    "                                 biLSTM_Attention.query_glove:aa1,\n",
    "                                 biLSTM_Attention.pos_ans_glove:bb1,\n",
    "                                 biLSTM_Attention.query_glove_seq_lengths:aa3,\n",
    "                                 biLSTM_Attention.pos_ans_glove_seq_lengths:bb3})\n",
    "    outputs.append(result[0].tolist());\n",
    "    progressBar(i,df_test.shape[0]);\n",
    "    batch_queries = [];\n",
    "    batch_passages = [];\n",
    "outputs = np.vstack(outputs)\n",
    "df_test['pred'] = outputs; # the smaller the score, the more positive_answer it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlS56pE97ILG"
   },
   "outputs": [],
   "source": [
    "# re-using code from Dileep\n",
    "outfilename = 'answer.tsv'\n",
    "with open(outfilename,\"w\",encoding=\"utf-8\") as fw:\n",
    "  import math\n",
    "  linelist = []\n",
    "  tempscores = []\n",
    "  for idx, row in tqdm(df_test.iterrows()):\n",
    "      tempscores.append(row['pred'])\n",
    "      #tempscores.append(str(row['pred']))\n",
    "      if((idx +1)%10==0):\n",
    "          tempscores-=np.min(tempscores);\n",
    "          tempscores = [math.exp(s) for s in tempscores];\n",
    "          expsum = sum(tempscores)\n",
    "          tempscores = [str(s/expsum) for s in tempscores]\n",
    "          scoreString = \"\\t\".join(tempscores)\n",
    "          qid = str(row[0])\n",
    "          fw.write(qid+\"\\t\"+scoreString+\"\\n\")\n",
    "          tempscores=[]\n",
    "      #if(idx%10000==0):\n",
    "      #    print(idx)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "0iKkfYSdPHM0",
    "evEnVCATnvOG",
    "8T70ZlM4LTnx",
    "67CAw6pZ_Rnm"
   ],
   "name": "MSAIC_2018.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
